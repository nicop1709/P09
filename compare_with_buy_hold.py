"""
Comparaison de toutes les strat√©gies avec Buy & Hold
Inclut : Buy & Hold, RandomForest, Logistic Regression, MiniROCKET, TabNet, TemporalFusionTransformer

Usage:
    python compare_with_buy_hold.py --timeframe 1h --start 2017-01-01 --end 2025-01-01
    python compare_with_buy_hold.py --csv-file btc_data.csv
"""

import argparse
import datetime as _dt
import glob
import json
import os
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, precision_score, confusion_matrix
import ta
import warnings
warnings.filterwarnings('ignore')

# Import centralized utilities
from backtest import Backtest
from utils import compute_advanced_features, buy_and_hold, fetch_ohlcv_binance


def print_confusion_matrix(y_true, y_pred, model_name=""):
    """Affiche une matrice de confusion format√©e"""
    try:
        cm = confusion_matrix(y_true, y_pred)
        
        # G√©rer les diff√©rents cas de taille de matrice
        if cm.size == 1:
            # Une seule classe pr√©dite
            if y_pred.sum() == 0:
                tn, fp, fn, tp = len(y_true) - y_true.sum(), 0, y_true.sum(), 0
            else:
                tn, fp, fn, tp = 0, 0, 0, len(y_true)
        elif cm.size == 4:
            tn, fp, fn, tp = cm.ravel()
        else:
            # Cas non binaire ou erreur
            print(f"\n   ‚ö†Ô∏è  Matrice de confusion non binaire pour {model_name}")
            return
        
        print(f"\n   üìä Matrice de confusion{(' - ' + model_name) if model_name else ''}:")
        print(f"   {'':>15} {'Pr√©dit 0':>12} {'Pr√©dit 1':>12}")
        print(f"   {'R√©el 0':>15} {tn:>12} {fp:>12}")
        print(f"   {'R√©el 1':>15} {fn:>12} {tp:>12}")
        print(f"   ‚Üí Vrais N√©gatifs (TN): {tn} | Faux Positifs (FP): {fp}")
        print(f"   ‚Üí Faux N√©gatifs (FN): {fn} | Vrais Positifs (TP): {tp}")
    except Exception as e:
        print(f"\n   ‚ö†Ô∏è  Erreur lors de l'affichage de la matrice de confusion: {e}")


# ============================================================
# PARAM√àTRES PAR D√âFAUT
# ============================================================
DEFAULT_START_YEAR = 2017
DEFAULT_END_YEAR = 2025
DEFAULT_TIMEFRAME = "1h"
DEFAULT_HORIZON_STEPS = 24
DEFAULT_FEE_ROUNDTRIP = 0.002
DEFAULT_TRAIN_RATIO = 0.85
DEFAULT_VALID_RATIO = 0.10
DEFAULT_RANDOM_SEED = 42
DEFAULT_LOOKBACK = 512  # For MiniRocket sequences
DEFAULT_NUM_KERNELS = 2000  # Reduced from 10000 to avoid OOM


# Backtest class and buy_and_hold function are now imported from backtest.py and utils.py

def create_advanced_features(df, horizon, threshold):
    """Cr√©e des features avanc√©es"""
    # Use centralized compute_advanced_features
    return compute_advanced_features(df, horizon=horizon, threshold=threshold)


def train_minirocket(df, y_train, y_valid, y_test, train_end, valid_end, test_df, lookback, random_seed, num_kernels=2000):
    """Entra√Æne un mod√®le MiniRocket sur s√©quences temporelles OHLCV
    
    Parameters
    ----------
    num_kernels : int, default=2000
        Number of kernels for MiniRocket. Reduced from 10000 to avoid OOM errors.
        Can be increased if more memory is available.
    """
    try:
        from sktime.transformations.panel.rocket import MiniRocket
        from sklearn.metrics import f1_score
    except ImportError as e:
        print("‚ö†Ô∏è  MiniRocket n√©cessite sktime qui n'est pas install√©.")
        print("   ‚Üí Pour installer: pip install sktime")
        print("   ‚Üí Ou avec toutes les d√©pendances: pip install 'sktime[all]'")
        print("   ‚Üí MiniRocket sera ignor√© pour cette ex√©cution.")
        return None, None, None, None

    print("üöÄ Pr√©paration des s√©quences temporelles MiniRocket...")

    # Normaliser les canaux
    close_mean, close_std = df["Close"].mean(), df["Close"].std()
    close_norm = (df["Close"] - close_mean) / (close_std + 1e-8)

    volume_mean, volume_std = df["Volume"].mean(), df["Volume"].std()
    volume_norm = (df["Volume"] - volume_mean) / (volume_std + 1e-8)

    returns = np.log(df["Close"] / df["Close"].shift(1)).fillna(0)

    hl_spread = (df["High"] - df["Low"]) / df["Close"]
    hl_mean, hl_std = hl_spread.mean(), hl_spread.std()
    hl_norm = (hl_spread - hl_mean) / (hl_std + 1e-8)

    # Cr√©er s√©quences de mani√®re plus m√©moire-efficace
    n_samples = len(df) - lookback + 1
    
    # Cr√©er directement le tableau final avec float32 pour √©conomiser m√©moire
    X_ts = np.zeros((n_samples, 4, lookback), dtype=np.float32)
    
    # Remplir avec une approche vectoris√©e plus efficace
    # Utiliser numpy pour cr√©er les fen√™tres glissantes
    close_arr = close_norm.values.astype(np.float32)
    volume_arr = volume_norm.values.astype(np.float32)
    returns_arr = returns.values.astype(np.float32)
    hl_arr = hl_norm.values.astype(np.float32)
    
    # Cr√©er les s√©quences de mani√®re vectoris√©e
    for i in range(n_samples):
        X_ts[i, 0, :] = close_arr[i:i+lookback]
        X_ts[i, 1, :] = volume_arr[i:i+lookback]
        X_ts[i, 2, :] = returns_arr[i:i+lookback]
        X_ts[i, 3, :] = hl_arr[i:i+lookback]

    # Aligner les targets
    y_full = np.concatenate([y_train, y_valid, y_test])

    train_end_seq = max(0, train_end - lookback + 1)
    valid_end_seq = max(0, valid_end - lookback + 1)

    if train_end_seq <= 0 or valid_end_seq <= train_end_seq:
        print("‚ö†Ô∏è  Pas assez de donn√©es pour MiniRocket")
        return None, None, None, None

    X_train_ts = X_ts[:train_end_seq]
    X_valid_ts = X_ts[train_end_seq:valid_end_seq]
    X_test_ts = X_ts[valid_end_seq:]

    y_train_seq = y_full[lookback-1:lookback-1+len(X_train_ts)]
    y_valid_seq = y_full[lookback-1+len(X_train_ts):lookback-1+len(X_train_ts)+len(X_valid_ts)]
    y_test_seq = y_full[lookback-1+len(X_train_ts)+len(X_valid_ts):lookback-1+len(X_train_ts)+len(X_valid_ts)+len(X_test_ts)]

    if len(y_train_seq) != len(X_train_ts) or len(y_test_seq) != len(X_test_ts):
        print("‚ö†Ô∏è  Alignement des targets √©chou√©")
        return None, None, None, None

    print(f"   ‚Üí Train sequences: {len(X_train_ts)}, Test sequences: {len(X_test_ts)}")

    # Transformer avec MiniRocket
    print(f"üî• Transformation MiniRocket ({num_kernels} kernels)...")
    rocket = MiniRocket(num_kernels=num_kernels, random_state=random_seed)
    
    # Fit et transform par batches pour √©conomiser la m√©moire
    print("   ‚Üí Fit MiniRocket...")
    rocket.fit(X_train_ts)
    
    print("   ‚Üí Transform train set...")
    X_train_transform = rocket.transform(X_train_ts)
    
    # Lib√©rer la m√©moire des donn√©es d'entra√Ænement si possible
    del X_train_ts
    
    print("   ‚Üí Transform validation set...")
    X_valid_transform = rocket.transform(X_valid_ts)
    del X_valid_ts
    
    print("   ‚Üí Transform test set...")
    X_test_transform = rocket.transform(X_test_ts)
    del X_test_ts

    # Standardiser
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train_transform)
    X_valid_std = scaler.transform(X_valid_transform)
    X_test_std = scaler.transform(X_test_transform)

    # Optimiser sur validation set
    print("üéØ Optimisation hyperparam√®tres...")
    best_f1 = 0
    best_clf = None
    best_threshold = 0.5

    for class_weight in [None, "balanced", {0: 1, 1: 2}, {0: 1, 1: 3}]:
        clf = RandomForestClassifier(
            n_estimators=200, max_depth=15, min_samples_leaf=5,
            class_weight=class_weight, random_state=random_seed, n_jobs=-1
        )
        clf.fit(X_train_std, y_train_seq)
        y_valid_prob = clf.predict_proba(X_valid_std)[:, 1]

        for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
            y_valid_pred = (y_valid_prob >= threshold).astype(int)
            f1 = f1_score(y_valid_seq, y_valid_pred)
            if f1 > best_f1:
                best_f1 = f1
                best_clf = clf
                best_threshold = threshold

    # Pr√©dictions sur test
    y_test_prob = best_clf.predict_proba(X_test_std)[:, 1]
    y_pred = (y_test_prob >= best_threshold).astype(int)

    # Aligner signaux avec test_df
    signals = np.zeros(len(test_df))
    probs = np.zeros(len(test_df))
    start_idx = lookback - 1
    if start_idx < len(test_df):
        end_idx = min(start_idx + len(y_pred), len(test_df))
        signals[start_idx:end_idx] = y_pred[:end_idx-start_idx]
        probs[start_idx:end_idx] = y_test_prob[:end_idx-start_idx]

    return signals, best_threshold, probs, y_test_seq


def train_tabnet(X_train, y_train, X_valid, y_valid, X_test, y_test, test_df, 
                 fee_roundtrip, random_seed):
    """Entra√Æne et optimise TabNet
    
    Returns
    -------
    capital, pnl, roi_annualized, thresh, bt_final, probs
        bt_final est l'objet Backtest final pour obtenir avg_trades
        probs sont les probabilit√©s pr√©dites pour le calcul de l'AUC
    """
    try:
        import torch
        from pytorch_tabnet.tab_model import TabNetClassifier
    except ImportError:
        print("‚ö†Ô∏è  TabNet n√©cessite pytorch-tabnet. Ignor√©.")
        return None, None, None, None, None
    
    print("üéØ Optimisation TabNet...")
    
    # Standardiser
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_valid_s = scaler.transform(X_valid)
    X_test_s = scaler.transform(X_test)
    
    # Configurations √† tester
    configs = [
        {"n_d": 32, "n_a": 32, "n_steps": 5, "lr": 1e-3, "gamma": 1.3, "patience": 25, "max_epochs": 150, "name": "Config1_standard"},
        {"n_d": 64, "n_a": 64, "n_steps": 6, "lr": 1.5e-3, "gamma": 1.4, "patience": 30, "max_epochs": 150, "name": "Config2_large"},
        {"n_d": 16, "n_a": 16, "n_steps": 4, "lr": 8e-4, "gamma": 1.2, "patience": 20, "max_epochs": 120, "name": "Config3_small"},
        {"n_d": 48, "n_a": 48, "n_steps": 7, "lr": 1.2e-3, "gamma": 1.35, "patience": 28, "max_epochs": 150, "name": "Config4_medium"},
    ]
    
    best_pnl = -float('inf')
    best_config = None
    best_capital = 0
    best_bt = None
    best_thresh = 0.5
    best_probs = None
    
    for i, config in enumerate(configs, 1):
        print(f"   Test {i}/{len(configs)} : {config['name']}")
        try:
            tabnet = TabNetClassifier(
                n_d=config['n_d'],
                n_a=config['n_a'],
                n_steps=config['n_steps'],
                gamma=config['gamma'],
                lambda_sparse=1e-4,
                optimizer_fn=torch.optim.Adam,
                optimizer_params=dict(lr=config['lr']),
                mask_type="sparsemax",
                seed=random_seed,
                verbose=0
            )
            
            tabnet.fit(
                X_train_s, y_train,
                eval_set=[(X_valid_s, y_valid)],
                eval_name=["valid"],
                eval_metric=["auc"],
                max_epochs=config.get('max_epochs', 150),
                patience=config['patience'],
                batch_size=1024,
                virtual_batch_size=256,
                num_workers=0,
                drop_last=False
            )
            
            p_test_tab = tabnet.predict_proba(X_test_s)[:, 1]
            
            # Test diff√©rents seuils
            thresholds = [0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65]
            for thresh in thresholds:
                pred_tab = (p_test_tab >= thresh).astype(int)
                bt = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_tab,
                            fee_roundtrip=fee_roundtrip, pct_capital=0.1, capital_init=1000)
                capital = bt.run()
                pnl = capital - 1000
                
                if pnl > best_pnl:
                    best_pnl = pnl
                    best_capital = capital
                    best_bt = bt
                    best_thresh = thresh
                    best_config = config
                    best_probs = p_test_tab
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur avec {config['name']}: {e}")
            continue
    
    if best_bt is not None:
        roi_annualized = best_bt.get_roi_annualized()
        return best_capital, best_pnl, roi_annualized, best_thresh, best_bt, best_probs
    else:
        return None, None, None, None, None, None


def train_tft(X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
              train_df, valid_df, feature_cols, fee_roundtrip, random_seed):
    """Entra√Æne et optimise TemporalFusionTransformer
    
    Returns
    -------
    capital, pnl, roi_annualized, thresh, bt_final, probs
        bt_final est l'objet Backtest final pour obtenir avg_trades
        probs sont les probabilit√©s pr√©dites pour le calcul de l'AUC
    """
    try:
        import torch
        import pytorch_lightning as pl
        import sys
        import types
        
        # pytorch-forecasting essaie d'importer lightning.pytorch
        # Cr√©er un alias pour que lightning.pytorch pointe vers pytorch_lightning
        if 'lightning' not in sys.modules:
            lightning_module = types.ModuleType('lightning')
            sys.modules['lightning'] = lightning_module
        if 'lightning.pytorch' not in sys.modules:
            sys.modules['lightning.pytorch'] = pl
        # Cr√©er aussi lightning.pytorch.trainer
        if 'lightning.pytorch.trainer' not in sys.modules:
            import pytorch_lightning.trainer as trainer_module
            sys.modules['lightning.pytorch.trainer'] = trainer_module
        # Cr√©er lightning.pytorch.utilities et ses sous-modules
        if 'lightning.pytorch.utilities' not in sys.modules:
            import pytorch_lightning.utilities as pl_utilities
            sys.modules['lightning.pytorch.utilities'] = pl_utilities
        if 'lightning.pytorch.utilities.argparse' not in sys.modules:
            try:
                import pytorch_lightning.utilities.argparse as pl_argparse
                sys.modules['lightning.pytorch.utilities.argparse'] = pl_argparse
            except ImportError:
                # Si argparse n'existe pas dans cette version, cr√©er un module avec les attributs n√©cessaires
                argparse_module = types.ModuleType('argparse')
                # Ajouter l'attribut _gpus_arg_default qui est utilis√© lors du chargement du checkpoint
                argparse_module._gpus_arg_default = lambda x: x
                sys.modules['lightning.pytorch.utilities.argparse'] = argparse_module
        
        from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer
        from pytorch_forecasting.metrics import MAE, RMSE, MAPE, SMAPE
        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
        
        # V√©rifier la compatibilit√© des versions
        pl_version = pl.__version__
        pl_major = int(pl_version.split('.')[0])
        if pl_major >= 2:
            print(f"   ‚ö†Ô∏è  Incompatibilit√© d√©tect√©e: pytorch-lightning {pl_version} >= 2.0")
            print("   ‚Üí pytorch-forecasting n√©cessite pytorch-lightning < 2.0")
            print("   ‚Üí Solution: pip install 'pytorch-lightning<2.0'")
            return None, None, None, None, None
        
        # Note: TFT h√©rite de lightning.pytorch.core.module.LightningModule
        # (nouveau chemin dans pytorch-lightning 1.8+), pas pytorch_lightning.core.LightningModule
        # Le Trainer de pytorch-lightning g√®re cela automatiquement
            
    except ImportError as e:
        print(f"‚ö†Ô∏è  TemporalFusionTransformer n√©cessite pytorch-forecasting et pytorch-lightning. Erreur: {e}")
        return None, None, None, None, None
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de l'import de TemporalFusionTransformer: {e}")
        return None, None, None, None, None
    
    print("üïê Optimisation TemporalFusionTransformer...")
    
    # Pr√©parer les donn√©es au format TimeSeriesDataSet
    # On a besoin de reconstruire les DataFrames avec time_idx et group_ids
    max_encoder_length = min(64, len(train_df) // 2)  # Longueur de l'historique (adaptatif)
    max_prediction_length = 1  # On pr√©dit 1 pas en avant
    
    if max_encoder_length < 10:
        print("   ‚ö†Ô∏è  Pas assez de donn√©es pour TFT (besoin d'au moins 20 √©chantillons)")
        return None, None, None, None, None
    
    # Cr√©er un DataFrame complet pour TFT
    df_full = pd.concat([train_df, valid_df, test_df], ignore_index=True)
    df_full = df_full.reset_index(drop=True)
    
    # Ajouter time_idx et group_ids
    df_full["time_idx"] = range(len(df_full))
    df_full["group_id"] = 0  # Une seule s√©rie temporelle
    
    # S√©parer train/valid/test avec les indices
    train_tft_df = df_full.iloc[:len(train_df)].copy()
    valid_tft_df = df_full.iloc[len(train_df):len(train_df)+len(valid_df)].copy()
    test_tft_df = df_full.iloc[len(train_df)+len(valid_df):].copy()
    
    # Cr√©er les datasets TimeSeriesDataSet
    try:
        training = TimeSeriesDataSet(
            train_tft_df,
            time_idx="time_idx",
            target="y",
            group_ids=["group_id"],
            min_encoder_length=max_encoder_length // 2,
            max_encoder_length=max_encoder_length,
            min_prediction_length=1,
            max_prediction_length=max_prediction_length,
            static_categoricals=[],
            time_varying_known_reals=feature_cols,
            time_varying_unknown_reals=["y"],
            target_normalizer=None,  # Pas de normalisation pour classification binaire
            add_relative_time_idx=True,
            add_target_scales=False,
            add_encoder_length=True,
        )
        
        validation = TimeSeriesDataSet.from_dataset(
            training,
            valid_tft_df,
            predict=True,
            stop_randomization=True,
        )
        
        # Cr√©er les dataloaders
        train_dataloader = training.to_dataloader(train=True, batch_size=64, num_workers=0)
        val_dataloader = validation.to_dataloader(train=False, batch_size=64, num_workers=0)
        
        # Configurer le mod√®le
        # TFT est con√ßu pour la r√©gression, on utilise MAE et on convertira en probabilit√©s apr√®s
        from pytorch_forecasting.metrics import MAE
        tft = TemporalFusionTransformer.from_dataset(
            training,
            learning_rate=0.001,
            hidden_size=64,
            attention_head_size=4,
            dropout=0.1,
            hidden_continuous_size=32,
            output_size=1,  # 1 sortie
            loss=MAE(),  # Utiliser une m√©trique PyTorch Lightning
            reduce_on_plateau_patience=4,
        )
        
        # Patch pour compatibilit√© avec pytorch-lightning 1.8+
        # Supprimer le hook d√©pr√©ci√© on_epoch_end du mod√®le TFT
        # Le mod√®le TFT de pytorch-forecasting peut avoir ce hook d√©pr√©ci√©
        if hasattr(tft, 'on_epoch_end'):
            # Remplacer par une m√©thode vide qui ne fait rien
            # Cela √©vite l'erreur "hook was removed in v1.8"
            def empty_on_epoch_end(*args, **kwargs):
                pass
            tft.on_epoch_end = types.MethodType(empty_on_epoch_end, tft)
        
        # Patch pour compatibilit√© avec pytorch-lightning 1.8+
        # D√©sactiver la v√©rification on_epoch_end dans le Trainer
        # IMPORTANT: Doit √™tre fait AVANT de cr√©er le Trainer
        try:
            import pytorch_lightning.trainer.configuration_validator as validator
            original_check = validator._check_on_epoch_start_end
            
            def noop_check(model):
                pass
            
            # Monkey-patch temporaire
            validator._check_on_epoch_start_end = noop_check
        except (ImportError, AttributeError) as e:
            # Si on ne peut pas patcher, on continue quand m√™me
            print(f"   ‚ö†Ô∏è  Impossible de patcher la v√©rification: {e}")
            validator = None
            original_check = None
        
        # Callbacks
        early_stop_callback = EarlyStopping(
            monitor="val_loss",
            min_delta=1e-4,
            patience=10,
            verbose=True,  # Afficher les messages d'early stopping
            mode="min"
        )
        
        # Callback personnalis√© pour afficher la progression avec timing
        import time
        class ProgressCallback(pl.Callback):
            def __init__(self):
                self.start_time = None
                self.epoch_times = []
                
            def on_train_start(self, trainer, pl_module):
                self.start_time = time.time()
                print(f"   ‚Üí D√©marrage de l'entra√Ænement (max {trainer.max_epochs} epochs)...")
                
            def on_train_epoch_end(self, trainer, pl_module):
                epoch = trainer.current_epoch
                metrics = trainer.callback_metrics
                train_loss = metrics.get('train_loss', None)
                val_loss = metrics.get('val_loss', None)
                
                # Calculer le temps pour cet epoch
                epoch_end_time = time.time()
                if hasattr(self, 'epoch_start_time'):
                    epoch_duration = epoch_end_time - self.epoch_start_time
                    self.epoch_times.append(epoch_duration)
                else:
                    epoch_duration = 0
                
                # Calculer le temps total et estimer le temps restant
                total_time = epoch_end_time - self.start_time
                if len(self.epoch_times) > 0:
                    avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
                    remaining_epochs = trainer.max_epochs - (epoch + 1)
                    estimated_remaining = avg_epoch_time * remaining_epochs
                    time_str = f" | Temps: {total_time/60:.1f}min | Restant: ~{estimated_remaining/60:.1f}min"
                else:
                    time_str = f" | Temps: {total_time/60:.1f}min"
                
                # Afficher les m√©triques
                loss_str = ""
                if train_loss is not None:
                    train_loss_val = train_loss.item() if hasattr(train_loss, 'item') else train_loss
                    loss_str += f"Train Loss: {train_loss_val:.6f}"
                if val_loss is not None:
                    val_loss_val = val_loss.item() if hasattr(val_loss, 'item') else val_loss
                    if loss_str:
                        loss_str += " | "
                    loss_str += f"Val Loss: {val_loss_val:.6f}"
                
                print(f"   ‚úì Epoch {epoch+1}/{trainer.max_epochs} - {loss_str}{time_str}")
                
            def on_train_epoch_start(self, trainer, pl_module):
                self.epoch_start_time = time.time()
                
            def on_train_end(self, trainer, pl_module):
                total_time = time.time() - self.start_time
                print(f"   ‚Üí Entra√Ænement termin√© en {total_time/60:.1f} minutes ({total_time:.0f} secondes)")
        
        progress_callback = ProgressCallback()
        
        # Trainer
        # D√©tecter le meilleur acc√©l√©rateur disponible (CUDA > MPS > CPU)
        if torch.cuda.is_available():
            accelerator = "gpu"
            devices = 1
            print("   ‚Üí Utilisation de CUDA (GPU NVIDIA)")
        elif torch.backends.mps.is_available():
            accelerator = "mps"
            devices = 1
            print("   ‚Üí Utilisation de MPS (GPU Apple Silicon)")
        else:
            accelerator = "cpu"
            devices = "auto"
            print("   ‚Üí Utilisation du CPU")
        
        trainer = pl.Trainer(
            max_epochs=10,
            accelerator=accelerator,
            devices=devices,
            gradient_clip_val=0.1,
            callbacks=[early_stop_callback, progress_callback],
            enable_progress_bar=True,  # Activer la barre de progression
            logger=False,  # Pas de logger externe, mais on garde les callbacks
        )
        
        # Afficher les informations sur le dataset
        print(f"   ‚Üí Dataset: {len(train_tft_df)} train, {len(valid_tft_df)} valid, {len(test_tft_df)} test")
        print(f"   ‚Üí Batch size: 64 | Max encoder length: {max_encoder_length}")
        
        # Entra√Æner
        print("   ‚Üí D√©marrage de l'entra√Ænement...")
        try:
            trainer.fit(
                tft,
                train_dataloaders=train_dataloader,
                val_dataloaders=val_dataloader,
            )
        except (TypeError, RuntimeError) as e:
            if "LightningModule" in str(e) or "must be a" in str(e) or "on_epoch_end" in str(e):
                print(f"   ‚ö†Ô∏è  Incompatibilit√© de versions d√©tect√©e: {e}")
                print("   ‚Üí pytorch-forecasting n√©cessite pytorch-lightning < 2.0")
                print("   ‚Üí Solution: pip install 'pytorch-lightning<2.0'")
                print("   ‚Üí Ou utilisez une version plus r√©cente de pytorch-forecasting si disponible")
                return None, None, None, None, None
            raise
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors de l'entra√Ænement TFT: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None, None, None
        finally:
            # Restaurer la fonction originale
            if validator is not None and original_check is not None:
                validator._check_on_epoch_start_end = original_check
        
        # Faire des pr√©dictions sur le test set
        # On doit cr√©er un dataset de test avec le bon format
        test_dataset = TimeSeriesDataSet.from_dataset(
            training,
            test_tft_df,
            predict=True,
            stop_randomization=True,
        )
        
        test_dataloader = test_dataset.to_dataloader(train=False, batch_size=128, num_workers=0)
        
        # Pr√©dictions
        print("   ‚Üí G√©n√©ration des pr√©dictions...")
        try:
            # Patch suppl√©mentaire pour la pr√©diction : s'assurer que on_epoch_end n'existe pas
            if hasattr(tft, 'on_epoch_end'):
                def empty_on_epoch_end(*args, **kwargs):
                    pass
                tft.on_epoch_end = types.MethodType(empty_on_epoch_end, tft)
            
            predictions = tft.predict(test_dataloader, return_y=False)
        except Exception as e:
            error_msg = str(e)
            if "on_epoch_end" in error_msg and "removed" in error_msg:
                print(f"   ‚ö†Ô∏è  Erreur de compatibilit√© PyTorch Lightning: {e}")
                print("   ‚Üí Tentative de contournement...")
                # Essayer de patcher plus agressivement
                try:
                    # Supprimer compl√®tement la m√©thode si possible
                    if hasattr(tft, 'on_epoch_end'):
                        def empty_on_epoch_end(*args, **kwargs):
                            pass
                        tft.on_epoch_end = types.MethodType(empty_on_epoch_end, tft)
                    # R√©essayer la pr√©diction
                    predictions = tft.predict(test_dataloader, return_y=False)
                except Exception as e2:
                    print(f"   ‚ö†Ô∏è  √âchec du contournement: {e2}")
                    return None, None, None, None, None
            else:
                print(f"   ‚ö†Ô∏è  Erreur lors des pr√©dictions: {e}")
                return None, None, None, None, None
        
        # Extraire les pr√©dictions et les convertir en probabilit√©s
        # predictions peut √™tre un tensor ou un array
        if isinstance(predictions, torch.Tensor):
            pred_tensor = predictions
        elif isinstance(predictions, (list, tuple)):
            # Si c'est une liste de tensors, les concat√©ner
            if len(predictions) > 0:
                pred_list = []
                for p in predictions:
                    if isinstance(p, torch.Tensor):
                        pred_list.append(p)
                    else:
                        pred_list.append(torch.tensor(p))
                pred_tensor = torch.cat(pred_list)
            else:
                print("   ‚ö†Ô∏è  Aucune pr√©diction g√©n√©r√©e")
                return None, None, None, None, None
        else:
            pred_tensor = torch.tensor(predictions)
        
        # Convertir en numpy
        # TFT pr√©dit des valeurs continues, on les normalise pour obtenir des probabilit√©s
        if isinstance(pred_tensor, torch.Tensor):
            pred_values = pred_tensor.cpu().numpy()
        else:
            pred_values = np.array(pred_tensor)
        
        # Normaliser les pr√©dictions pour les convertir en probabilit√©s
        # TFT pr√©dit des valeurs continues, on doit les convertir en probabilit√©s
        if len(pred_values) > 0:
            # Diagnostic des valeurs brutes
            print(f"   üìä Diagnostic TFT - Valeurs brutes avant conversion:")
            print(f"      ‚Üí Min: {np.min(pred_values):.6f} | Max: {np.max(pred_values):.6f} | Mean: {np.mean(pred_values):.6f} | Std: {np.std(pred_values):.6f}")
            
            # Nettoyer les NaN et infini dans les valeurs brutes
            pred_values = np.nan_to_num(pred_values, nan=0.0, posinf=1e6, neginf=-1e6)
            
            # M√©thode am√©lior√©e : utiliser la distribution des pr√©dictions
            # 1. Centrer et normaliser selon la distribution
            pred_mean = np.mean(pred_values)
            pred_std = np.std(pred_values) + 1e-8
            
            # 2. Standardiser (z-score)
            pred_standardized = (pred_values - pred_mean) / pred_std
            
            # 3. Appliquer sigmoid avec un facteur d'√©chelle pour am√©liorer la sensibilit√©
            # Utiliser un facteur plus petit pour avoir une courbe sigmoid plus douce
            scale_factor = 0.5  # Ajuster selon les donn√©es
            pred_probs = 1 / (1 + np.exp(-pred_standardized * scale_factor))
            
            # 4. Ajuster pour √©viter les valeurs extr√™mes (0 ou 1)
            pred_probs = np.clip(pred_probs, 0.01, 0.99)
            
            # 5. Recalibrer pour avoir une distribution plus √©quilibr√©e
            # √âtaler la distribution si elle est trop concentr√©e
            prob_range = pred_probs.max() - pred_probs.min()
            if prob_range < 0.4:  # Si la plage est trop petite, √©taler
                if prob_range > 1e-6:  # √âviter division par z√©ro
                    # √âtaler proportionnellement sur une plage plus large [0.15, 0.85]
                    # Pr√©server l'ordre relatif mais √©taler pour avoir plus de variance
                    pred_probs = 0.15 + (pred_probs - pred_probs.min()) / prob_range * 0.7
                else:
                    # Si toutes les valeurs sont identiques, cr√©er une distribution autour de 0.5
                    # avec un peu de variance bas√©e sur l'index (pour pr√©server l'ordre temporel)
                    indices = np.arange(len(pred_probs))
                    noise = (indices % 10) / 10.0 * 0.3 - 0.15  # Variation cyclique
                    pred_probs = 0.5 + noise
                    pred_probs = np.clip(pred_probs, 0.15, 0.85)
            
            # Nettoyer √† nouveau les NaN qui pourraient √™tre apparus
            pred_probs = np.nan_to_num(pred_probs, nan=0.5, posinf=0.99, neginf=0.01)
            pred_probs = np.clip(pred_probs, 0.0, 1.0)
        else:
            pred_probs = np.array([])
        
        # Aplatir si n√©cessaire
        if len(pred_probs.shape) > 1:
            pred_probs = pred_probs.flatten()
        
        # S'assurer qu'on a le bon nombre de pr√©dictions
        # TFT peut ne pas pr√©dire pour tous les points (besoin d'historique)
        # On prend les derni√®res pr√©dictions disponibles
        if len(pred_probs) > len(test_df):
            # Prendre les derni√®res pr√©dictions
            pred_probs = pred_probs[-len(test_df):]
        elif len(pred_probs) < len(test_df):
            # Padding au d√©but avec 0.5 (probabilit√© neutre)
            padding = np.full(len(test_df) - len(pred_probs), 0.5)
            pred_probs = np.concatenate([padding, pred_probs])
        
        # S'assurer que les probabilit√©s sont dans [0, 1]
        pred_probs = np.clip(pred_probs, 0.0, 1.0)
        
        # Optimiser le seuil avec une plage plus large et plus fine pour TFT
        best_pnl = -float('inf')
        best_capital = 0
        best_bt = None
        best_thresh = 0.5
        
        # Plage de seuils plus large et plus fine pour TFT
        thresholds = np.arange(0.20, 0.81, 0.05)  # De 0.20 √† 0.80 par pas de 0.05
        for thresh in thresholds:
            pred_tft = (pred_probs >= thresh).astype(int)
            bt = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_tft,
                        fee_roundtrip=fee_roundtrip, pct_capital=0.1, capital_init=1000)
            capital = bt.run()
            pnl = capital - 1000
            
            if pnl > best_pnl:
                best_pnl = pnl
                best_capital = capital
                best_bt = bt
                best_thresh = thresh
        
        if best_bt is not None:
            roi_annualized = best_bt.get_roi_annualized()
            return best_capital, best_pnl, roi_annualized, best_thresh, best_bt, pred_probs
        else:
            return None, None, None, None, None, None
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur lors de l'entra√Ænement TFT: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None


def load_tft_from_checkpoint_and_predict(X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                                         train_df, valid_df, feature_cols, fee_roundtrip, random_seed,
                                         checkpoint_path=None):
    """Charge un mod√®le TFT depuis un checkpoint et fait les pr√©dictions sans r√©entra√Æner
    
    Parameters
    ----------
    checkpoint_path : str, optional
        Chemin vers le checkpoint. Si None, utilise le plus r√©cent dans checkpoints/
    
    Returns
    -------
    capital, pnl, roi_annualized, thresh, bt_final, probs
        bt_final est l'objet Backtest final pour obtenir avg_trades
        probs sont les probabilit√©s pr√©dites pour le calcul de l'AUC
    """
    try:
        import torch
        import pytorch_lightning as pl
        import sys
        import types
        import os
        import glob
        import pandas as pd
        import numpy as np
        
        # pytorch-forecasting essaie d'importer lightning.pytorch
        # Cr√©er un alias pour que lightning.pytorch pointe vers pytorch_lightning
        if 'lightning' not in sys.modules:
            lightning_module = types.ModuleType('lightning')
            sys.modules['lightning'] = lightning_module
        if 'lightning.pytorch' not in sys.modules:
            sys.modules['lightning.pytorch'] = pl
        # Cr√©er aussi lightning.pytorch.trainer
        if 'lightning.pytorch.trainer' not in sys.modules:
            import pytorch_lightning.trainer as trainer_module
            sys.modules['lightning.pytorch.trainer'] = trainer_module
        # Cr√©er lightning.pytorch.utilities et ses sous-modules
        if 'lightning.pytorch.utilities' not in sys.modules:
            import pytorch_lightning.utilities as pl_utilities
            sys.modules['lightning.pytorch.utilities'] = pl_utilities
        if 'lightning.pytorch.utilities.argparse' not in sys.modules:
            try:
                import pytorch_lightning.utilities.argparse as pl_argparse
                sys.modules['lightning.pytorch.utilities.argparse'] = pl_argparse
            except ImportError:
                # Si argparse n'existe pas dans cette version, cr√©er un module avec les attributs n√©cessaires
                argparse_module = types.ModuleType('argparse')
                # Ajouter l'attribut _gpus_arg_default qui est utilis√© lors du chargement du checkpoint
                argparse_module._gpus_arg_default = lambda x: x
                sys.modules['lightning.pytorch.utilities.argparse'] = argparse_module
        
        from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer
        from backtest import Backtest
        
        # Trouver le checkpoint le plus r√©cent si non sp√©cifi√©
        if checkpoint_path is None:
            checkpoint_dir = "checkpoints"
            if os.path.exists(checkpoint_dir):
                checkpoints = glob.glob(os.path.join(checkpoint_dir, "*.ckpt"))
                if checkpoints:
                    # Trier par date de modification (le plus r√©cent en premier)
                    checkpoints.sort(key=os.path.getmtime, reverse=True)
                    checkpoint_path = checkpoints[0]
                    print(f"   ‚Üí Utilisation du checkpoint le plus r√©cent: {os.path.basename(checkpoint_path)}")
                else:
                    print("   ‚ö†Ô∏è  Aucun checkpoint trouv√© dans checkpoints/")
                    return None, None, None, None, None
            else:
                print("   ‚ö†Ô∏è  Dossier checkpoints/ introuvable")
                return None, None, None, None, None
        else:
            if not os.path.exists(checkpoint_path):
                print(f"   ‚ö†Ô∏è  Checkpoint introuvable: {checkpoint_path}")
                return None, None, None, None, None
        
        print(f"   ‚Üí Chargement du mod√®le depuis: {checkpoint_path}")
        
        # Pr√©parer les donn√©es au format TimeSeriesDataSet (m√™me code que train_tft)
        max_encoder_length = min(64, len(train_df) // 2)
        max_prediction_length = 1
        
        if max_encoder_length < 10:
            print("   ‚ö†Ô∏è  Pas assez de donn√©es pour TFT")
            return None, None, None, None, None
        
        # Cr√©er un DataFrame complet pour TFT
        df_full = pd.concat([train_df, valid_df, test_df], ignore_index=True)
        df_full = df_full.reset_index(drop=True)
        
        # Ajouter time_idx et group_ids
        df_full["time_idx"] = range(len(df_full))
        df_full["group_id"] = 0
        
        # S√©parer train/valid/test
        train_tft_df = df_full.iloc[:len(train_df)].copy()
        valid_tft_df = df_full.iloc[len(train_df):len(train_df)+len(valid_df)].copy()
        test_tft_df = df_full.iloc[len(train_df)+len(valid_df):].copy()
        
        # Recr√©er le dataset d'entra√Ænement (n√©cessaire pour charger le mod√®le)
        training = TimeSeriesDataSet(
            train_tft_df,
            time_idx="time_idx",
            target="y",
            group_ids=["group_id"],
            min_encoder_length=max_encoder_length // 2,
            max_encoder_length=max_encoder_length,
            min_prediction_length=1,
            max_prediction_length=max_prediction_length,
            static_categoricals=[],
            time_varying_known_reals=feature_cols,
            time_varying_unknown_reals=["y"],
            target_normalizer=None,
            add_relative_time_idx=True,
            add_target_scales=False,
            add_encoder_length=True,
        )
        
        # Charger le mod√®le depuis le checkpoint
        print("   ‚Üí Chargement du mod√®le TFT...")
        try:
            tft = TemporalFusionTransformer.load_from_checkpoint(checkpoint_path)
            print("   ‚úì Mod√®le charg√© avec succ√®s")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors du chargement du mod√®le: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None, None, None
        
        # Patch pour compatibilit√© avec pytorch-lightning 1.8+
        # Supprimer le hook d√©pr√©ci√© on_epoch_end du mod√®le TFT
        # Il faut patcher √† la fois l'instance ET la classe de base
        if hasattr(tft, 'on_epoch_end'):
            def empty_on_epoch_end(*args, **kwargs):
                pass
            tft.on_epoch_end = types.MethodType(empty_on_epoch_end, tft)
            # Patching de la classe de base pour √©viter la v√©rification du validator
            tft_class = type(tft)
            # Parcourir la hi√©rarchie des classes pour trouver et supprimer on_epoch_end
            for cls in tft_class.__mro__:
                if hasattr(cls, 'on_epoch_end') and 'on_epoch_end' in cls.__dict__:
                    # Remplacer par une m√©thode vide dans la classe
                    setattr(cls, 'on_epoch_end', lambda self, *args, **kwargs: None)
            print("   ‚úì Patch on_epoch_end appliqu√© (instance et classes de base)")
        
        # Cr√©er le dataset de test
        test_dataset = TimeSeriesDataSet.from_dataset(
            training,
            test_tft_df,
            predict=True,
            stop_randomization=True,
        )
        
        test_dataloader = test_dataset.to_dataloader(train=False, batch_size=128, num_workers=0)
        
        # Patch pour compatibilit√© avec pytorch-lightning 1.8+
        # D√©sactiver la v√©rification on_epoch_end dans le Trainer avant predict()
        validator = None
        original_check = None
        predict_callback_patch = None
        original_on_predict_epoch_end = None
        
        try:
            import pytorch_lightning.trainer.configuration_validator as validator
            original_check = validator._check_on_epoch_start_end
            
            def noop_check(model):
                pass
            
            # Monkey-patch temporaire
            validator._check_on_epoch_start_end = noop_check
        except (ImportError, AttributeError) as e:
            # Si on ne peut pas patcher, on continue quand m√™me
            validator = None
            original_check = None
        
        # Patch PredictCallback pour corriger la signature on_predict_epoch_end
        try:
            from pytorch_forecasting.models.base._base_model import PredictCallback
            if hasattr(PredictCallback, 'on_predict_epoch_end'):
                original_on_predict_epoch_end = PredictCallback.on_predict_epoch_end
                # Nouvelle signature compatible avec pytorch-lightning 1.8+
                def patched_on_predict_epoch_end(self, trainer, pl_module, outputs, *args, **kwargs):
                    # Appeler la m√©thode originale si elle existe, sinon ne rien faire
                    if original_on_predict_epoch_end:
                        try:
                            # Essayer avec l'ancienne signature (3 args)
                            return original_on_predict_epoch_end(self, trainer, pl_module)
                        except TypeError:
                            # Si √ßa √©choue, essayer avec la nouvelle signature (4 args)
                            return original_on_predict_epoch_end(self, trainer, pl_module, outputs)
                    return None
                PredictCallback.on_predict_epoch_end = patched_on_predict_epoch_end
                predict_callback_patch = PredictCallback
        except (ImportError, AttributeError) as e:
            # Si on ne peut pas patcher, on continue quand m√™me
            predict_callback_patch = None
            original_on_predict_epoch_end = None
        
        # Pr√©dictions
        print("   ‚Üí G√©n√©ration des pr√©dictions...")
        try:
            predictions = tft.predict(test_dataloader, return_y=False)
        except Exception as e:
            error_msg = str(e)
            if "on_epoch_end" in error_msg and "removed" in error_msg:
                print(f"   ‚ö†Ô∏è  Erreur de compatibilit√© PyTorch Lightning: {e}")
                print("   ‚Üí Tentative de contournement...")
                try:
                    if hasattr(tft, 'on_epoch_end'):
                        def empty_on_epoch_end(*args, **kwargs):
                            pass
                        tft.on_epoch_end = types.MethodType(empty_on_epoch_end, tft)
                    predictions = tft.predict(test_dataloader, return_y=False)
                except Exception as e2:
                    print(f"   ‚ö†Ô∏è  √âchec du contournement: {e2}")
                    import traceback
                    traceback.print_exc()
                    return None, None, None, None, None
            else:
                print(f"   ‚ö†Ô∏è  Erreur lors des pr√©dictions: {e}")
                import traceback
                traceback.print_exc()
                return None, None, None, None, None
        finally:
            # Restaurer les fonctions originales
            if validator is not None and original_check is not None:
                validator._check_on_epoch_start_end = original_check
            if predict_callback_patch is not None and original_on_predict_epoch_end is not None:
                predict_callback_patch.on_predict_epoch_end = original_on_predict_epoch_end
        
        # Convertir les pr√©dictions en probabilit√©s (m√™me code que train_tft)
        if isinstance(predictions, torch.Tensor):
            pred_tensor = predictions
        elif isinstance(predictions, (list, tuple)):
            if len(predictions) > 0:
                pred_list = []
                for p in predictions:
                    if isinstance(p, torch.Tensor):
                        pred_list.append(p)
                    else:
                        pred_list.append(torch.tensor(p))
                pred_tensor = torch.cat(pred_list)
            else:
                print("   ‚ö†Ô∏è  Aucune pr√©diction g√©n√©r√©e")
                return None, None, None, None, None
        else:
            pred_tensor = torch.tensor(predictions)
        
        if isinstance(pred_tensor, torch.Tensor):
            pred_values = pred_tensor.cpu().numpy()
        else:
            pred_values = np.array(pred_tensor)
        
        # Normaliser les pr√©dictions pour les convertir en probabilit√©s (m√™me m√©thode am√©lior√©e)
        if len(pred_values) > 0:
            # Diagnostic des valeurs brutes
            print(f"   üìä Diagnostic TFT - Valeurs brutes avant conversion:")
            print(f"      ‚Üí Min: {np.min(pred_values):.6f} | Max: {np.max(pred_values):.6f} | Mean: {np.mean(pred_values):.6f} | Std: {np.std(pred_values):.6f}")
            
            # Nettoyer les NaN et infini dans les valeurs brutes
            pred_values = np.nan_to_num(pred_values, nan=0.0, posinf=1e6, neginf=-1e6)
            
            # M√©thode am√©lior√©e : utiliser la distribution des pr√©dictions
            pred_mean = np.mean(pred_values)
            pred_std = np.std(pred_values) + 1e-8
            
            # Standardiser (z-score)
            pred_standardized = (pred_values - pred_mean) / pred_std
            
            # Appliquer sigmoid avec un facteur d'√©chelle
            scale_factor = 0.5
            pred_probs = 1 / (1 + np.exp(-pred_standardized * scale_factor))
            
            # Ajuster pour √©viter les valeurs extr√™mes
            pred_probs = np.clip(pred_probs, 0.01, 0.99)
            
            # Recalibrer pour avoir une distribution plus √©quilibr√©e
            # √âtaler la distribution si elle est trop concentr√©e
            prob_range = pred_probs.max() - pred_probs.min()
            if prob_range < 0.4:  # Si la plage est trop petite, √©taler
                if prob_range > 1e-6:  # √âviter division par z√©ro
                    # √âtaler proportionnellement sur une plage plus large [0.15, 0.85]
                    pred_probs = 0.15 + (pred_probs - pred_probs.min()) / prob_range * 0.7
                else:
                    # Si toutes les valeurs sont identiques, cr√©er une distribution autour de 0.5
                    indices = np.arange(len(pred_probs))
                    noise = (indices % 10) / 10.0 * 0.3 - 0.15  # Variation cyclique
                    pred_probs = 0.5 + noise
                    pred_probs = np.clip(pred_probs, 0.15, 0.85)
            
            # Nettoyer √† nouveau les NaN qui pourraient √™tre apparus
            pred_probs = np.nan_to_num(pred_probs, nan=0.5, posinf=0.99, neginf=0.01)
            pred_probs = np.clip(pred_probs, 0.0, 1.0)
        else:
            pred_probs = np.array([])
        
        # Aplatir si n√©cessaire
        if len(pred_probs.shape) > 1:
            pred_probs = pred_probs.flatten()
        
        # Ajuster la longueur des pr√©dictions
        if len(pred_probs) > len(test_df):
            pred_probs = pred_probs[-len(test_df):]
        elif len(pred_probs) < len(test_df):
            padding = np.full(len(test_df) - len(pred_probs), 0.5)
            pred_probs = np.concatenate([padding, pred_probs])
        
        pred_probs = np.clip(pred_probs, 0.0, 1.0)
        
        # Optimiser le seuil avec une plage plus large et plus fine pour TFT
        best_pnl = -float('inf')
        best_capital = 0
        best_bt = None
        best_thresh = 0.5
        
        # Plage de seuils plus large et plus fine pour TFT
        thresholds = np.arange(0.20, 0.81, 0.05)  # De 0.20 √† 0.80 par pas de 0.05
        for thresh in thresholds:
            pred_tft = (pred_probs >= thresh).astype(int)
            bt = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_tft,
                        fee_roundtrip=fee_roundtrip, pct_capital=0.1, capital_init=1000)
            capital = bt.run()
            pnl = capital - 1000
            
            if pnl > best_pnl:
                best_pnl = pnl
                best_capital = capital
                best_bt = bt
                best_thresh = thresh
        
        if best_bt is not None:
            roi_annualized = best_bt.get_roi_annualized()
            print(f"   ‚úì Pr√©dictions g√©n√©r√©es avec succ√®s (seuil optimal: {best_thresh:.2f})")
            return best_capital, best_pnl, roi_annualized, best_thresh, best_bt, pred_probs
        else:
            return None, None, None, None, None, None
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur lors du chargement et des pr√©dictions TFT: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None


def train_tft_with_importance(X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                                train_df, valid_df, feature_cols, fee_roundtrip, random_seed,
                                return_importance=True, top_k=20):
    """
    Version √©tendue de train_tft qui retourne aussi la feature importance.
    
    Parameters
    ----------
    return_importance : bool
        Si True, calcule et retourne la feature importance
    top_k : int
        Nombre de top features √† retourner dans l'importance
    
    Returns
    -------
    tuple
        (capital, pnl, roi_annualized, thresh, bt, importance_results)
        o√π importance_results est un dict avec les r√©sultats de feature importance
        ou None si return_importance=False ou en cas d'erreur
    """
    # Appeler la fonction train_tft normale
    result = train_tft(X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                       train_df, valid_df, feature_cols, fee_roundtrip, random_seed)
    
    if result[0] is None or not return_importance:
        # Si l'entra√Ænement a √©chou√© ou si on ne veut pas l'importance
        return result + (None,)
    
    # Si l'entra√Ænement a r√©ussi, on doit r√©entra√Æner ou r√©cup√©rer le mod√®le
    # Pour l'instant, on retourne None pour l'importance
    # L'utilisateur devra utiliser extract_tft_feature_importance s√©par√©ment
    # avec un mod√®le sauvegard√©
    print("   ‚ÑπÔ∏è  Pour obtenir la feature importance, utilisez extract_tft_feature_importance()")
    print("   ‚ÑπÔ∏è  avec le mod√®le TFT sauvegard√© apr√®s l'entra√Ænement.")
    return result + (None,), None, None


def extract_tft_feature_importance(tft, training_dataset, feature_cols, top_k=20):
    """
    Extrait l'importance des features depuis un mod√®le TFT entra√Æn√©.
    
    Utilise deux m√©thodes :
    1. Variable Selection Network (VSN) : poids attribu√©s aux variables
    2. Attention weights : importance des pas temporels
    
    Parameters
    ----------
    tft : TemporalFusionTransformer
        Mod√®le TFT entra√Æn√©
    training_dataset : TimeSeriesDataSet
        Dataset d'entra√Ænement utilis√© pour TFT
    feature_cols : list
        Liste des noms des features
    top_k : int
        Nombre de top features √† retourner (par d√©faut 20)
    
    Returns
    -------
    dict
        Dictionnaire contenant :
        - 'vsn_importance' : DataFrame avec les importances VSN
        - 'attention_stats' : Statistiques sur les poids d'attention
        - 'summary' : R√©sum√© des top features
    """
    try:
        import torch
        
        # Mettre le mod√®le en mode √©valuation
        tft.eval()
        
        results = {}
        
        # ============================================================
        # 1. Variable Selection Network (VSN) Importance
        # ============================================================
        try:
            # Acc√©der au Variable Selection Network
            # Le VSN est dans tft.variable_selection
            if hasattr(tft, 'variable_selection'):
                vsn = tft.variable_selection
                
                # Extraire les poids des variables connues (time_varying_known_reals)
                # Ces poids sont dans les couches de s√©lection de variables
                vsn_weights = {}
                
                # TFT utilise des embeddings pour les variables continues
                # On peut extraire les poids depuis les couches de transformation
                if hasattr(vsn, 'variable_selection_weights'):
                    # Si disponible directement
                    weights = vsn.variable_selection_weights
                    if weights is not None:
                        if isinstance(weights, torch.Tensor):
                            weights_np = weights.detach().cpu().numpy()
                            if len(weights_np.shape) > 1:
                                # Moyenner sur les dimensions suppl√©mentaires
                                weights_np = weights_np.mean(axis=tuple(range(1, len(weights_np.shape))))
                            for i, feat in enumerate(feature_cols[:len(weights_np)]):
                                vsn_weights[feat] = float(weights_np[i])
                
                # Alternative : utiliser les gradients ou les poids des embeddings
                if not vsn_weights and hasattr(vsn, 'continuous_variable_selection'):
                    cvs = vsn.continuous_variable_selection
                    if hasattr(cvs, 'weight'):
                        weights = cvs.weight
                        if weights is not None:
                            weights_np = weights.detach().cpu().numpy()
                            # Prendre la norme des poids pour chaque feature
                            if len(weights_np.shape) >= 2:
                                # Norme L2 des poids pour chaque feature
                                feature_norms = np.linalg.norm(weights_np, axis=-1)
                                if len(feature_norms.shape) > 1:
                                    feature_norms = feature_norms.mean(axis=0)
                                for i, feat in enumerate(feature_cols[:len(feature_norms)]):
                                    vsn_weights[feat] = float(feature_norms[i])
                
                # Si toujours pas de poids, utiliser une m√©thode de permutation
                if not vsn_weights:
                    print("   ‚ö†Ô∏è  Impossible d'extraire les poids VSN directement, utilisation de la permutation importance...")
                    vsn_weights = _compute_permutation_importance_tft(tft, training_dataset, feature_cols)
                
                # Cr√©er un DataFrame avec les importances
                if vsn_weights:
                    df_vsn = pd.DataFrame({
                        'feature': list(vsn_weights.keys()),
                        'importance': list(vsn_weights.values())
                    }).sort_values('importance', ascending=False)
                    
                    # Normaliser les importances entre 0 et 1
                    if df_vsn['importance'].max() > df_vsn['importance'].min():
                        df_vsn['importance_normalized'] = (
                            (df_vsn['importance'] - df_vsn['importance'].min()) /
                            (df_vsn['importance'].max() - df_vsn['importance'].min())
                        )
                    else:
                        df_vsn['importance_normalized'] = 1.0
                    
                    results['vsn_importance'] = df_vsn
                    results['top_vsn_features'] = df_vsn.head(top_k)
                else:
                    print("   ‚ö†Ô∏è  Aucun poids VSN extrait")
                    results['vsn_importance'] = pd.DataFrame(columns=['feature', 'importance', 'importance_normalized'])
                    
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors de l'extraction VSN: {e}")
            results['vsn_importance'] = pd.DataFrame(columns=['feature', 'importance', 'importance_normalized'])
        
        # ============================================================
        # 2. Attention Weights (importance temporelle)
        # ============================================================
        try:
            # Extraire les poids d'attention depuis le m√©canisme d'attention
            if hasattr(tft, 'attention') and hasattr(tft.attention, 'attention_weights'):
                attention_weights = tft.attention.attention_weights
                if attention_weights is not None:
                    # Les poids d'attention montrent l'importance des pas temporels
                    # On peut calculer des statistiques
                    if isinstance(attention_weights, torch.Tensor):
                        att_np = attention_weights.detach().cpu().numpy()
                        results['attention_stats'] = {
                            'mean': float(att_np.mean()),
                            'std': float(att_np.std()),
                            'min': float(att_np.min()),
                            'max': float(att_np.max()),
                            'shape': list(att_np.shape)
                        }
                    else:
                        results['attention_stats'] = {'note': 'Attention weights non disponibles sous forme tensorielle'}
            else:
                results['attention_stats'] = {'note': 'M√©canisme d\'attention non accessible directement'}
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors de l'extraction des attention weights: {e}")
            results['attention_stats'] = {'error': str(e)}
        
        # ============================================================
        # 3. R√©sum√©
        # ============================================================
        if 'vsn_importance' in results and len(results['vsn_importance']) > 0:
            top_features = results['vsn_importance'].head(top_k)
            results['summary'] = {
                'total_features': len(feature_cols),
                'top_k': top_k,
                'top_features': top_features['feature'].tolist(),
                'top_importances': top_features['importance'].tolist()
            }
        else:
            results['summary'] = {'note': 'Aucune feature importance disponible'}
        
        return results
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur g√©n√©rale lors de l'extraction de feature importance: {e}")
        import traceback
        traceback.print_exc()
        return {
            'vsn_importance': pd.DataFrame(columns=['feature', 'importance', 'importance_normalized']),
            'attention_stats': {'error': str(e)},
            'summary': {'error': str(e)}
        }


def print_importance_summary(importance_results, top_k=20):
    """
    Affiche un r√©sum√© textuel de la feature importance.
    
    Parameters
    ----------
    importance_results : dict
        R√©sultats retourn√©s par extract_tft_feature_importance
    top_k : int
        Nombre de top features √† afficher
    """
    print("\n" + "="*80)
    print("üìä FEATURE IMPORTANCE - Temporal Fusion Transformer")
    print("="*80)
    
    # VSN Importance
    if 'vsn_importance' in importance_results and len(importance_results['vsn_importance']) > 0:
        df_importance = importance_results['vsn_importance']
        print(f"\nüîù Top {top_k} Features (Variable Selection Network):")
        print("-" * 80)
        print(f"{'Rang':<6} {'Feature':<40} {'Importance':<15} {'Normalis√©e':<15}")
        print("-" * 80)
        
        for idx, row in df_importance.head(top_k).iterrows():
            print(f"{idx+1:<6} {row['feature']:<40} {row['importance']:<15.6f} {row['importance_normalized']:<15.4f}")
        
        print(f"\nüìà Statistiques:")
        print(f"   ‚Üí Total features: {len(df_importance)}")
        print(f"   ‚Üí Importance moyenne: {df_importance['importance'].mean():.6f}")
        print(f"   ‚Üí Importance m√©diane: {df_importance['importance'].median():.6f}")
        print(f"   ‚Üí Importance max: {df_importance['importance'].max():.6f}")
        print(f"   ‚Üí Importance min: {df_importance['importance'].min():.6f}")
    else:
        print("\n‚ö†Ô∏è  Aucune donn√©e VSN disponible")
    
    # Attention Stats
    if 'attention_stats' in importance_results:
        att_stats = importance_results['attention_stats']
        if 'error' not in att_stats and 'note' not in att_stats:
            print(f"\nüéØ Statistiques des Attention Weights:")
            print(f"   ‚Üí Moyenne: {att_stats.get('mean', 'N/A')}")
            print(f"   ‚Üí √âcart-type: {att_stats.get('std', 'N/A')}")
            print(f"   ‚Üí Min: {att_stats.get('min', 'N/A')}")
            print(f"   ‚Üí Max: {att_stats.get('max', 'N/A')}")
            if 'shape' in att_stats:
                print(f"   ‚Üí Shape: {att_stats['shape']}")
        else:
            print(f"\n‚ö†Ô∏è  Attention weights: {att_stats.get('note', att_stats.get('error', 'Non disponible'))}")
    
    # Summary
    if 'summary' in importance_results and 'error' not in importance_results['summary']:
        summary = importance_results['summary']
        if 'top_features' in summary:
            print(f"\nüìã R√©sum√©:")
            print(f"   ‚Üí Top {summary.get('top_k', top_k)} features identifi√©es")
            print(f"   ‚Üí Total features analys√©es: {summary.get('total_features', 'N/A')}")
    
    print("="*80 + "\n")


def _compute_permutation_importance_tft(tft, training_dataset, feature_cols, n_samples=100, random_seed=42):
    """
    Calcule l'importance par permutation pour TFT.
    Cette m√©thode est plus co√ªteuse mais fonctionne toujours.
    """
    try:
        import torch
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        
        # Cr√©er un dataloader avec un √©chantillon
        dataloader = training_dataset.to_dataloader(train=False, batch_size=min(32, len(training_dataset)), num_workers=0)
        
        # Calculer la baseline (pr√©diction normale)
        tft.eval()
        baseline_preds = []
        with torch.no_grad():
            for batch in dataloader:
                x, y = batch
                pred = tft(x)
                if isinstance(pred, (list, tuple)):
                    pred = pred[0]
                baseline_preds.append(pred.cpu().numpy())
        
        baseline_preds = np.concatenate(baseline_preds)
        baseline_loss = np.mean(np.abs(baseline_preds))
        
        # Pour chaque feature, calculer l'impact de la permutation
        feature_importance = {}
        
        print(f"   ‚Üí Calcul de l'importance par permutation pour {len(feature_cols)} features...")
        print(f"   ‚Üí (Cela peut prendre du temps, √©chantillon de {n_samples} points)")
        
        # Limiter le nombre de features test√©es pour la performance
        n_features_to_test = min(len(feature_cols), 30)  # Limiter √† 30 features max
        features_to_test = np.random.choice(feature_cols, size=n_features_to_test, replace=False)
        
        for i, feat_name in enumerate(features_to_test):
            try:
                # Trouver l'index de la feature dans le dataset
                feat_idx = feature_cols.index(feat_name)
                
                # Permuter cette feature dans les donn√©es
                permuted_preds = []
                with torch.no_grad():
                    for batch in dataloader:
                        x, y = batch
                        # Permuter la feature
                        if isinstance(x, dict):
                            # Si x est un dictionnaire, trouver la cl√© appropri√©e
                            for key in x.keys():
                                if x[key].shape[-1] > feat_idx:
                                    x_perm = x.copy()
                                    # Permuter les valeurs de cette feature
                                    permuted_values = x_perm[key][:, :, feat_idx].clone()
                                    permuted_values = permuted_values[torch.randperm(len(permuted_values))]
                                    x_perm[key][:, :, feat_idx] = permuted_values
                                    x = x_perm
                                    break
                        else:
                            # Si x est un tensor, permuter la dimension de la feature
                            if x.shape[-1] > feat_idx:
                                permuted_values = x[:, :, feat_idx].clone()
                                permuted_values = permuted_values[torch.randperm(len(permuted_values))]
                                x[:, :, feat_idx] = permuted_values
                        
                        pred = tft(x)
                        if isinstance(pred, (list, tuple)):
                            pred = pred[0]
                        permuted_preds.append(pred.cpu().numpy())
                
                permuted_preds = np.concatenate(permuted_preds)
                permuted_loss = np.mean(np.abs(permuted_preds))
                
                # L'importance est la diff√©rence de performance
                importance = abs(permuted_loss - baseline_loss)
                feature_importance[feat_name] = importance
                
                if (i + 1) % 5 == 0:
                    print(f"   ‚Üí Trait√© {i+1}/{n_features_to_test} features...")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur pour feature {feat_name}: {e}")
                feature_importance[feat_name] = 0.0
        
        # Pour les features non test√©es, mettre une valeur par d√©faut
        for feat in feature_cols:
            if feat not in feature_importance:
                feature_importance[feat] = 0.0
        
        return feature_importance
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur dans permutation importance: {e}")
        # Retourner des valeurs par d√©faut
        return {feat: 0.0 for feat in feature_cols}


# ============================================================
# MAIN
# ============================================================
def load_models_config(config_path="models_config.json"):
    """Charge la configuration des mod√®les depuis un fichier JSON"""
    if not os.path.exists(config_path):
        print(f"‚ö†Ô∏è  Fichier de configuration {config_path} introuvable, utilisation de tous les mod√®les par d√©faut")
        # Retourner tous les mod√®les activ√©s par d√©faut
        return {
            "buy_hold": True,
            "rf_baseline": True,
            "rf_advanced": True,
            "logistic_regression": True,
            "minirocket": True,
            "tabnet": True,
            "tft": True
        }
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Cr√©er un dictionnaire simple {nom_model: enabled}
        models_dict = {}
        for model in config.get("models", []):
            models_dict[model["name"]] = model.get("enabled", True)
        
        return models_dict
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors du chargement de {config_path}: {e}")
        print("   ‚Üí Utilisation de tous les mod√®les par d√©faut")
        return {
            "buy_hold": True,
            "rf_baseline": True,
            "rf_advanced": True,
            "logistic_regression": True,
            "minirocket": True,
            "tabnet": True,
            "tft": True
        }


def save_session_results(results_df, session_params, session_dir="session"):
    """Sauvegarde les r√©sultats de la session dans un CSV horodat√©
    
    Parameters
    ----------
    results_df : pd.DataFrame
        DataFrame contenant les r√©sultats de chaque strat√©gie
    session_params : dict
        Dictionnaire contenant les param√®tres de la session
    session_dir : str
        R√©pertoire o√π sauvegarder les r√©sultats (default: "session")
    """
    # Cr√©er le r√©pertoire s'il n'existe pas
    os.makedirs(session_dir, exist_ok=True)
    
    # G√©n√©rer le timestamp pour le nom de fichier
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"session_{timestamp}.csv"
    filepath = os.path.join(session_dir, filename)
    
    # Cr√©er un DataFrame √©tendu avec les m√©tadonn√©es
    # Ajouter les param√®tres de session comme colonnes suppl√©mentaires
    results_extended = results_df.copy()
    
    # Ajouter les m√©tadonn√©es de session
    for key, value in session_params.items():
        results_extended[f"param_{key}"] = value
    
    # Ajouter le timestamp
    results_extended["session_timestamp"] = timestamp
    
    # Sauvegarder
    results_extended.to_csv(filepath, index=False)
    
    print(f"\nüíæ R√©sultats sauvegard√©s dans : {filepath}")
    
    return filepath


def main():
    parser = argparse.ArgumentParser(description="Comparaison Buy & Hold vs ML Models")
    parser.add_argument("--timeframe", type=str, default=DEFAULT_TIMEFRAME,
                       help=f"Timeframe (default: {DEFAULT_TIMEFRAME})")
    parser.add_argument("--models-config", type=str, default="models_config.json",
                       help="Path to JSON file listing models to compare (default: models_config.json)")
    parser.add_argument("--start", type=str, default=None,
                       help=f"Start date YYYY-MM-DD (default: {DEFAULT_START_YEAR}-01-01)")
    parser.add_argument("--end", type=str, default=None,
                       help=f"End date YYYY-MM-DD (default: {DEFAULT_END_YEAR}-01-01)")
    parser.add_argument("--csv-file", type=str, default=None,
                       help="CSV file path (if provided, uses this instead of fetching)")
    parser.add_argument("--horizon", type=int, default=DEFAULT_HORIZON_STEPS,
                       help=f"Horizon steps (default: {DEFAULT_HORIZON_STEPS})")
    parser.add_argument("--fee", type=float, default=DEFAULT_FEE_ROUNDTRIP,
                       help=f"Roundtrip fee (default: {DEFAULT_FEE_ROUNDTRIP})")
    parser.add_argument("--train-ratio", type=float, default=DEFAULT_TRAIN_RATIO,
                       help=f"Train ratio (default: {DEFAULT_TRAIN_RATIO})")
    parser.add_argument("--valid-ratio", type=float, default=DEFAULT_VALID_RATIO,
                       help=f"Validation ratio (default: {DEFAULT_VALID_RATIO})")
    parser.add_argument("--lookback", type=int, default=DEFAULT_LOOKBACK,
                       help=f"Lookback for MiniROCKET (default: {DEFAULT_LOOKBACK})")
    parser.add_argument("--num-kernels", type=int, default=DEFAULT_NUM_KERNELS,
                       help=f"Number of kernels for MiniROCKET (default: {DEFAULT_NUM_KERNELS}, reduced from 10000 to avoid OOM)")
    parser.add_argument("--random-seed", type=int, default=DEFAULT_RANDOM_SEED,
                       help=f"Random seed (default: {DEFAULT_RANDOM_SEED})")
    
    args = parser.parse_args()
    
    # G√©n√©rer le timestamp de la session
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Charger la configuration des mod√®les
    models_config = load_models_config(args.models_config)
    print(f"\nüìã Configuration des mod√®les charg√©e depuis {args.models_config}")
    enabled_models = [name for name, enabled in models_config.items() if enabled]
    print(f"   ‚Üí Mod√®les activ√©s: {', '.join(enabled_models) if enabled_models else 'Aucun'}")
    if not enabled_models:
        print("   ‚ùå Aucun mod√®le activ√© dans la configuration!")
        return
    
    # Set parameters
    TIMEFRAME = args.timeframe
    HORIZON_STEPS = args.horizon
    FEE_ROUNDTRIP = args.fee
    THRESH = FEE_ROUNDTRIP
    TRAIN_RATIO = args.train_ratio
    VALID_RATIO = args.valid_ratio
    LOOKBACK = args.lookback
    NUM_KERNELS = args.num_kernels
    RANDOM_SEED = args.random_seed
    
    np.random.seed(RANDOM_SEED)
    try:
        import torch
        torch.manual_seed(RANDOM_SEED)
    except ImportError:
        pass
    
    # Determine dates
    if args.start:
        start_dt = _dt.datetime.fromisoformat(args.start)
        START_YEAR = start_dt.year
    else:
        START_YEAR = DEFAULT_START_YEAR
        start_dt = _dt.datetime(START_YEAR, 1, 1)
    
    if args.end:
        end_dt = _dt.datetime.fromisoformat(args.end)
        END_YEAR = end_dt.year
    else:
        END_YEAR = DEFAULT_END_YEAR
        end_dt = _dt.datetime(END_YEAR, 1, 1)
    
    CACHE = f"btc_usdc_{TIMEFRAME}_{START_YEAR}_{END_YEAR}.csv"
    
    print("\n" + "="*80)
    print("üìä COMPARAISON COMPL√àTE : Buy & Hold vs ML Models")
    print("="*80)
    print(f"Timeframe: {TIMEFRAME}, P√©riode: {start_dt.date()} √† {end_dt.date()}")

    # 1. Chargement
    print("\nüì• Chargement des donn√©es...")
    if args.csv_file and os.path.exists(args.csv_file):
        df = pd.read_csv(args.csv_file, parse_dates=["Timestamp"])
        df = df.dropna().reset_index(drop=True)
        print(f"   ‚Üí {len(df)} lignes charg√©es depuis {args.csv_file}")
    elif os.path.exists(CACHE):
        df = pd.read_csv(CACHE, parse_dates=["Timestamp"])
        df = df.dropna().reset_index(drop=True)
        print(f"   ‚Üí {len(df)} lignes charg√©es depuis {CACHE}")
    else:
        print(f"   ‚Üí Fichier {CACHE} introuvable, r√©cup√©ration depuis Binance...")
        try:
            df = fetch_ohlcv_binance(
                pair="BTC/USDC",
                timeframe=TIMEFRAME,
                start=start_dt,
                end=end_dt,
            )
            # Save cache
            df.to_csv(CACHE, index=False)
            print(f"   ‚Üí {len(df)} lignes r√©cup√©r√©es et sauvegard√©es dans {CACHE}")
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la r√©cup√©ration: {e}")
            return

    # 2. Feature engineering
    print("\nüîß Feature engineering...")
    # Garder une copie du df original pour MiniROCKET (besoin OHLCV brut)
    df_original = df.copy()
    df = create_advanced_features(df, horizon=HORIZON_STEPS, threshold=THRESH)

    feature_cols = [col for col in df.columns if col not in
                    ["Timestamp", "future_close", "roi_H", "y", "Open"] and
                    df[col].dtype in [np.float64, np.int64]]

    df = df.replace([np.inf, -np.inf], np.nan)
    df_model = df.dropna(subset=feature_cols + ["y"]).reset_index(drop=True)
    print(f"   ‚Üí {len(df_model)} lignes apr√®s features")

    # 3. Split
    n = len(df_model)
    train_end = int(n * TRAIN_RATIO)
    valid_end = int(n * (TRAIN_RATIO + VALID_RATIO))

    train_df = df_model.iloc[:train_end].copy()
    valid_df = df_model.iloc[train_end:valid_end].copy()
    test_df = df_model.iloc[valid_end:].copy()

    X_train = train_df[feature_cols].values
    y_train = train_df["y"].values
    X_valid = valid_df[feature_cols].values
    y_valid = valid_df["y"].values
    X_test = test_df[feature_cols].values
    y_test = test_df["y"].values

    print(f"   ‚Üí Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)} lignes")
    print(f"   ‚Üí P√©riode test: {test_df['Timestamp'].iloc[0]} √† {test_df['Timestamp'].iloc[-1]}")

    # ========== BUY & HOLD ==========
    # Buy & Hold est toujours calcul√© pour la comparaison, m√™me si d√©sactiv√©
    capital_bh, qty_bh, price_start, price_end = buy_and_hold(
        test_df, capital_init=1000, pct_capital=0.1, fee_roundtrip=FEE_ROUNDTRIP
    )
    pnl_bh = capital_bh - 1000
    roi_bh = (price_end - price_start) / price_start * 100
    from utils import calculate_roi_annualized
    roi_annualized_bh = calculate_roi_annualized(1000, capital_bh, test_df)
    # Buy & Hold = 1 trade (entr√©e au d√©but, sortie √† la fin)
    if "Timestamp" in test_df.columns:
        start_date = pd.to_datetime(test_df["Timestamp"].iloc[0])
        end_date = pd.to_datetime(test_df["Timestamp"].iloc[-1])
        days_bh = (end_date - start_date).days
        if days_bh <= 0:
            days_bh = 1
        avg_trades_bh = 1.0 / days_bh  # 1 trade pour toute la p√©riode
    else:
        avg_trades_bh = 0.0

    if models_config.get("buy_hold", True):
        print("\n" + "="*80)
        print("üí∞ STRAT√âGIE 1 : BUY & HOLD (Baseline)")
        print("="*80)
        print(f"Prix d√©but  : {price_start:.2f}‚Ç¨")
        print(f"Prix fin    : {price_end:.2f}‚Ç¨")
        print(f"ROI Bitcoin : {roi_bh:+.2f}%")
        print(f"Capital final : {capital_bh:.2f}‚Ç¨")
        print(f"PnL : {pnl_bh:+.2f}‚Ç¨")
        print(f"ROI annualized : {roi_annualized_bh:.2f}%")
        print(f"Trades/jour moyen : {avg_trades_bh:.4f}")

    # ========== RANDOM FOREST BASELINE ==========
    if models_config.get("rf_baseline", True):
        print("\n" + "="*80)
        print("üå≤ STRAT√âGIE 2 : RandomForest Baseline (minimum features Open, High, Low, Close, Volume)")
        print("="*80)

        basic_features = [
            "Open", "High", "Low", "Close", "Volume"
        ]

        X_train_basic = train_df[basic_features].values
        X_test_basic = test_df[basic_features].values

        rf_basic = RandomForestClassifier(
            n_estimators=800, max_depth=10, min_samples_leaf=10,
            class_weight="balanced", random_state=RANDOM_SEED, n_jobs=-1, verbose=0
        )
        rf_basic.fit(X_train_basic, y_train)

        p_test_basic = rf_basic.predict_proba(X_test_basic)[:, 1]
        pred_basic = (p_test_basic >= 0.5).astype(int)
        
        # Calculer les m√©triques
        auc_basic = roc_auc_score(y_test, p_test_basic)
        precision_basic = precision_score(y_test, pred_basic, zero_division=0)
        f1_basic = f1_score(y_test, pred_basic)

        bt_basic = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_basic,
                           fee_roundtrip=FEE_ROUNDTRIP, pct_capital=0.1, capital_init=1000)
        capital_basic = bt_basic.run()
        pnl_basic = capital_basic - 1000
        roi_annualized_basic = bt_basic.get_roi_annualized()
        avg_trades_basic = bt_basic.get_avg_trades_per_day()

        print(f"Capital final : {capital_basic:.2f}‚Ç¨")
        print(f"PnL : {pnl_basic:+.2f}‚Ç¨")
        print(f"ROI annualized : {roi_annualized_basic:.2f}%")
        print(f"AUC : {auc_basic:.4f}")
        print(f"Precision : {precision_basic:.4f}")
        print(f"F1-score : {f1_basic:.4f}")
        print_confusion_matrix(y_test, pred_basic, "RandomForest Baseline")
        print(f"Trades/jour moyen : {avg_trades_basic:.4f}")
        print(f"vs Buy&Hold : {pnl_basic - pnl_bh:+.2f}‚Ç¨")
    else:
        capital_basic = 0
        pnl_basic = -1000
        roi_annualized_basic = -100.0
        avg_trades_basic = 0.0

    # ========== RANDOM FOREST ADVANCED ==========
    if models_config.get("rf_advanced", True):
        print("\n" + "="*80)
        print("üå≤‚ú® STRAT√âGIE 3 : RandomForest + Features Avanc√©es (74 features)")
        print("="*80)

        rf_adv = RandomForestClassifier(
            n_estimators=1000, max_depth=15, min_samples_leaf=5,
            class_weight="balanced", max_features="sqrt",
            random_state=RANDOM_SEED, n_jobs=-1, verbose=0
        )
        rf_adv.fit(X_train, y_train)

        p_test_adv = rf_adv.predict_proba(X_test)[:, 1]

        # Optimisation seuil
        best_pnl_adv = -float('inf')
        best_thresh_adv = 0.5
        best_bt_adv = None
        for thresh in np.arange(0.40, 0.66, 0.02):
            pred_adv_thresh = (p_test_adv >= thresh).astype(int)
            bt = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_adv_thresh,
                         fee_roundtrip=FEE_ROUNDTRIP, pct_capital=0.1, capital_init=1000)
            cap = bt.run()
            pnl = cap - 1000
            if pnl > best_pnl_adv:
                best_pnl_adv = pnl
                best_thresh_adv = thresh
                capital_adv = cap
                best_bt_adv = bt

        roi_annualized_adv = best_bt_adv.get_roi_annualized() if best_bt_adv else 0.0
        avg_trades_adv = best_bt_adv.get_avg_trades_per_day() if best_bt_adv else 0.0
        # Calculer les m√©triques
        auc_adv = roc_auc_score(y_test, p_test_adv)
        pred_adv_optimal = (p_test_adv >= best_thresh_adv).astype(int)
        precision_adv = precision_score(y_test, pred_adv_optimal, zero_division=0)
        f1_adv = f1_score(y_test, pred_adv_optimal)
        print(f"Seuil optimal : {best_thresh_adv:.2f}")
        print(f"Capital final : {capital_adv:.2f}‚Ç¨")
        print(f"PnL : {best_pnl_adv:+.2f}‚Ç¨")
        print(f"ROI annualized : {roi_annualized_adv:.2f}%")
        print(f"AUC : {auc_adv:.4f}")
        print(f"Precision : {precision_adv:.4f}")
        print(f"F1-score : {f1_adv:.4f}")
        print_confusion_matrix(y_test, pred_adv_optimal, "RandomForest Advanced")
        print(f"Trades/jour moyen : {avg_trades_adv:.4f}")
        print(f"vs Buy&Hold : {best_pnl_adv - pnl_bh:+.2f}‚Ç¨")
    else:
        capital_adv = 0
        best_pnl_adv = -1000
        roi_annualized_adv = -100.0
        avg_trades_adv = 0.0
        best_thresh_adv = 0.5

    # ========== LOGISTIC REGRESSION ==========
    if models_config.get("logistic_regression", True):
        print("\n" + "="*80)
        print("üìä STRAT√âGIE 4 : Logistic Regression (74 features)")
        print("="*80)

        # Normalisation n√©cessaire pour la r√©gression logistique
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        lr = LogisticRegression(
            class_weight="balanced",
            max_iter=1000,
            random_state=RANDOM_SEED,
            n_jobs=-1
        )
        lr.fit(X_train_scaled, y_train)

        p_test_lr = lr.predict_proba(X_test_scaled)[:, 1]

        # Optimisation seuil
        best_pnl_lr = -float('inf')
        best_thresh_lr = 0.5
        best_bt_lr = None
        for thresh in np.arange(0.40, 0.66, 0.02):
            pred_lr_thresh = (p_test_lr >= thresh).astype(int)
            bt = Backtest(df_bt=test_df.reset_index(drop=True), signals=pred_lr_thresh,
                         fee_roundtrip=FEE_ROUNDTRIP, pct_capital=0.1, capital_init=1000)
            cap = bt.run()
            pnl = cap - 1000
            if pnl > best_pnl_lr:
                best_pnl_lr = pnl
                best_thresh_lr = thresh
                capital_lr = cap
                best_bt_lr = bt

        roi_annualized_lr = best_bt_lr.get_roi_annualized() if best_bt_lr else 0.0
        avg_trades_lr = best_bt_lr.get_avg_trades_per_day() if best_bt_lr else 0.0
        # Calculer les m√©triques
        auc_lr = roc_auc_score(y_test, p_test_lr)
        pred_lr_optimal = (p_test_lr >= best_thresh_lr).astype(int)
        precision_lr = precision_score(y_test, pred_lr_optimal, zero_division=0)
        f1_lr = f1_score(y_test, pred_lr_optimal)
        print(f"Seuil optimal : {best_thresh_lr:.2f}")
        print(f"Capital final : {capital_lr:.2f}‚Ç¨")
        print(f"PnL : {best_pnl_lr:+.2f}‚Ç¨")
        print(f"ROI annualized : {roi_annualized_lr:.2f}%")
        print(f"AUC : {auc_lr:.4f}")
        print(f"Precision : {precision_lr:.4f}")
        print(f"F1-score : {f1_lr:.4f}")
        print_confusion_matrix(y_test, pred_lr_optimal, "Logistic Regression")
        print(f"Trades/jour moyen : {avg_trades_lr:.4f}")
        print(f"vs Buy&Hold : {best_pnl_lr - pnl_bh:+.2f}‚Ç¨")
    else:
        capital_lr = 0
        best_pnl_lr = -1000
        roi_annualized_lr = -100.0
        avg_trades_lr = 0.0
        best_thresh_lr = 0.5

    # ========== MINIROCKET ==========
    if models_config.get("minirocket", True):
        print("\n" + "="*80)
        print("üöÄ STRAT√âGIE 5 : MiniRocket (Time Series)")
        print("="*80)

        # Utiliser df AVANT feature engineering pour MiniRocket (besoin OHLCV brut)
        # Utiliser df_original sauvegard√© plus t√¥t
        df_raw = df_original.copy()
        df_raw = df_raw.dropna().reset_index(drop=True)

        # Cr√©er les m√™mes features de base pour avoir les targets
        df_raw = create_advanced_features(df_raw, horizon=HORIZON_STEPS, threshold=THRESH)
        df_raw = df_raw.replace([np.inf, -np.inf], np.nan)
        df_raw = df_raw.dropna(subset=feature_cols + ["y"]).reset_index(drop=True)

        # Split avec les m√™mes indices
        n_raw = len(df_raw)
        train_end_raw = int(n_raw * TRAIN_RATIO)
        valid_end_raw = int(n_raw * (TRAIN_RATIO + VALID_RATIO))

        y_train_raw = df_raw.iloc[:train_end_raw]["y"].values
        y_valid_raw = df_raw.iloc[train_end_raw:valid_end_raw]["y"].values
        y_test_raw = df_raw.iloc[valid_end_raw:]["y"].values

        result_mr = train_minirocket(
            df_raw, y_train_raw, y_valid_raw, y_test_raw,
            train_end_raw, valid_end_raw,
            df_raw.iloc[valid_end_raw:].copy(),
            lookback=LOOKBACK,
            random_seed=RANDOM_SEED,
            num_kernels=NUM_KERNELS
        )

        if result_mr[0] is not None:
            signals_mr, thresh_mr, probs_mr, y_test_seq_mr = result_mr
            bt_mr = Backtest(df_bt=df_raw.iloc[valid_end_raw:].reset_index(drop=True), signals=signals_mr,
                            fee_roundtrip=FEE_ROUNDTRIP, pct_capital=0.1, capital_init=1000)
            capital_mr = bt_mr.run()
            pnl_mr = capital_mr - 1000
            roi_annualized_mr = bt_mr.get_roi_annualized()
            avg_trades_mr = bt_mr.get_avg_trades_per_day()
            
            # Calculer les m√©triques (aligner les probabilit√©s avec y_test_seq)
            # Les probabilit√©s sont align√©es avec test_df, mais y_test_seq est plus court
            # On prend les probabilit√©s correspondantes
            probs_aligned = probs_mr[LOOKBACK-1:LOOKBACK-1+len(y_test_seq_mr)] if len(probs_mr) >= LOOKBACK-1+len(y_test_seq_mr) else probs_mr[-len(y_test_seq_mr):]
            signals_aligned = signals_mr[LOOKBACK-1:LOOKBACK-1+len(y_test_seq_mr)] if len(signals_mr) >= LOOKBACK-1+len(y_test_seq_mr) else signals_mr[-len(y_test_seq_mr):]
            
            if len(probs_aligned) == len(y_test_seq_mr) and len(probs_aligned) > 0:
                auc_mr = roc_auc_score(y_test_seq_mr, probs_aligned)
            else:
                auc_mr = 0.0
            
            if len(signals_aligned) == len(y_test_seq_mr) and len(signals_aligned) > 0:
                precision_mr = precision_score(y_test_seq_mr, signals_aligned, zero_division=0)
                f1_mr = f1_score(y_test_seq_mr, signals_aligned)
            else:
                precision_mr = 0.0
                f1_mr = 0.0

            print(f"Seuil optimal : {thresh_mr:.2f}")
            print(f"Capital final : {capital_mr:.2f}‚Ç¨")
            print(f"PnL : {pnl_mr:+.2f}‚Ç¨")
            print(f"ROI annualized : {roi_annualized_mr:.2f}%")
            print(f"AUC : {auc_mr:.4f}")
            print(f"Precision : {precision_mr:.4f}")
            print(f"F1-score : {f1_mr:.4f}")
            print_confusion_matrix(y_test_seq_mr, signals_aligned, "MiniROCKET")
            print(f"Trades/jour moyen : {avg_trades_mr:.4f}")
            print(f"vs Buy&Hold : {pnl_mr - pnl_bh:+.2f}‚Ç¨")
        else:
            capital_mr = 0
            pnl_mr = -1000
            roi_annualized_mr = -100.0
            avg_trades_mr = 0.0
            signals_mr = None
            print("‚ö†Ô∏è  MiniRocket non disponible (sktime non install√©)")
            print("   ‚Üí Pour installer: pip install sktime")
            print("   ‚Üí Ou avec toutes les d√©pendances: pip install 'sktime[all]'")
    else:
        capital_mr = 0
        pnl_mr = -1000
        roi_annualized_mr = -100.0
        avg_trades_mr = 0.0
        signals_mr = None

    # ========== TABNET ==========
    if models_config.get("tabnet", True):
        print("\n" + "="*80)
        print("üß† STRAT√âGIE 6 : TabNet (Optimized)")
        print("="*80)

        result_tab = train_tabnet(
            X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
            fee_roundtrip=FEE_ROUNDTRIP,
            random_seed=RANDOM_SEED
        )
        
        if result_tab[0] is not None:
            capital_tab, pnl_tab, roi_annualized_tab, thresh_tab, bt_tab, probs_tab = result_tab
            avg_trades_tab = bt_tab.get_avg_trades_per_day()
            # Calculer les m√©triques
            auc_tab = roc_auc_score(y_test, probs_tab) if probs_tab is not None else 0.0
            pred_tab_optimal = (probs_tab >= thresh_tab).astype(int) if probs_tab is not None else np.zeros(len(y_test), dtype=int)
            precision_tab = precision_score(y_test, pred_tab_optimal, zero_division=0) if probs_tab is not None else 0.0
            f1_tab = f1_score(y_test, pred_tab_optimal) if probs_tab is not None else 0.0
            
            print(f"Seuil optimal : {thresh_tab:.2f}")
            print(f"Capital final : {capital_tab:.2f}‚Ç¨")
            print(f"PnL : {pnl_tab:+.2f}‚Ç¨")
            print(f"ROI annualized : {roi_annualized_tab:.2f}%")
            print(f"AUC : {auc_tab:.4f}")
            print(f"Precision : {precision_tab:.4f}")
            print(f"F1-score : {f1_tab:.4f}")
            print_confusion_matrix(y_test, pred_tab_optimal, "TabNet")
            print(f"Trades/jour moyen : {avg_trades_tab:.4f}")
            print(f"vs Buy&Hold : {pnl_tab - pnl_bh:+.2f}‚Ç¨")
        else:
            capital_tab = 0
            pnl_tab = -1000
            roi_annualized_tab = -100.0
            avg_trades_tab = 0.0
            print("‚ö†Ô∏è  TabNet non disponible (pytorch-tabnet non install√©)")
    else:
        capital_tab = 0
        pnl_tab = -1000
        roi_annualized_tab = -100.0
        avg_trades_tab = 0.0

    # ========== TEMPORAL FUSION TRANSFORMER ==========
    if models_config.get("tft", True):
        print("\n" + "="*80)
        print("üïê STRAT√âGIE 7 : TemporalFusionTransformer (TFT)")
        print("="*80)

        # Essayer d'abord de charger depuis un checkpoint si disponible
        checkpoint_dir = "checkpoints"
        use_checkpoint = False
        checkpoint_path = None
        
        # V√©rifier si un checkpoint existe
        if os.path.exists(checkpoint_dir):
            checkpoints = glob.glob(os.path.join(checkpoint_dir, "*.ckpt"))
            if checkpoints:
                # Utiliser le plus r√©cent
                checkpoints.sort(key=os.path.getmtime, reverse=True)
                checkpoint_path = checkpoints[0]
                use_checkpoint = True
                print(f"   ‚Üí Checkpoint trouv√©: {os.path.basename(checkpoint_path)}")
                print("   ‚Üí Tentative de chargement depuis le checkpoint (skip entra√Ænement)...")
        
        if use_checkpoint:
            # Charger depuis le checkpoint
            result_tft = load_tft_from_checkpoint_and_predict(
                X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                train_df, valid_df, feature_cols,
                fee_roundtrip=FEE_ROUNDTRIP,
                random_seed=RANDOM_SEED,
                checkpoint_path=checkpoint_path
            )
            
            # Si le chargement a √©chou√©, essayer d'entra√Æner
            if result_tft[0] is None:
                print("   ‚ö†Ô∏è  √âchec du chargement depuis le checkpoint")
                print("   ‚Üí D√©marrage de l'entra√Ænement...")
                result_tft = train_tft(
                    X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                    train_df, valid_df, feature_cols,
                    fee_roundtrip=FEE_ROUNDTRIP,
                    random_seed=RANDOM_SEED
                )
        else:
            # Pas de checkpoint, entra√Æner normalement
            result_tft = train_tft(
                X_train, y_train, X_valid, y_valid, X_test, y_test, test_df,
                train_df, valid_df, feature_cols,
                fee_roundtrip=FEE_ROUNDTRIP,
                random_seed=RANDOM_SEED
            )
        
        if result_tft[0] is not None:
            capital_tft, pnl_tft, roi_annualized_tft, thresh_tft, bt_tft, probs_tft = result_tft
            avg_trades_tft = bt_tft.get_avg_trades_per_day()
            
            # Diagnostic : afficher la distribution des probabilit√©s
            if probs_tft is not None and len(probs_tft) > 0:
                print(f"   üìä Diagnostic TFT - Distribution des probabilit√©s:")
                print(f"      ‚Üí Min: {np.min(probs_tft):.4f} | Max: {np.max(probs_tft):.4f} | Mean: {np.mean(probs_tft):.4f} | Median: {np.median(probs_tft):.4f}")
                print(f"      ‚Üí Seuil optimal trouv√©: {thresh_tft:.4f}")
                print(f"      ‚Üí Pr√©dictions >= seuil: {(probs_tft >= thresh_tft).sum()} / {len(probs_tft)} ({(probs_tft >= thresh_tft).sum()/len(probs_tft)*100:.1f}%)")
                print(f"      ‚Üí Distribution: <0.3: {(probs_tft < 0.3).sum()}, 0.3-0.5: {((probs_tft >= 0.3) & (probs_tft < 0.5)).sum()}, 0.5-0.7: {((probs_tft >= 0.5) & (probs_tft < 0.7)).sum()}, >=0.7: {(probs_tft >= 0.7).sum()}")
            
            # Calculer les m√©triques (aligner les probabilit√©s avec y_test)
            # Les probabilit√©s TFT peuvent √™tre plus courtes que y_test
            if probs_tft is not None and len(probs_tft) > 0:
                # Nettoyer les NaN et infini
                probs_tft_clean = np.array(probs_tft)
                probs_tft_clean = np.nan_to_num(probs_tft_clean, nan=0.5, posinf=0.99, neginf=0.01)
                probs_tft_clean = np.clip(probs_tft_clean, 0.0, 1.0)
                
                if len(probs_tft_clean) == len(y_test):
                    # Filtrer les NaN si pr√©sents
                    valid_mask = ~(np.isnan(probs_tft_clean) | np.isnan(y_test))
                    if valid_mask.sum() > 0:
                        auc_tft = roc_auc_score(y_test[valid_mask], probs_tft_clean[valid_mask])
                    else:
                        auc_tft = 0.5
                    y_test_aligned = y_test
                    probs_tft_aligned = probs_tft_clean
                elif len(probs_tft_clean) < len(y_test):
                    # Prendre les derni√®res valeurs de y_test correspondantes
                    y_test_subset = y_test[-len(probs_tft_clean):]
                    valid_mask = ~(np.isnan(probs_tft_clean) | np.isnan(y_test_subset))
                    if valid_mask.sum() > 0:
                        auc_tft = roc_auc_score(y_test_subset[valid_mask], probs_tft_clean[valid_mask])
                    else:
                        auc_tft = 0.5
                    y_test_aligned = y_test_subset
                    probs_tft_aligned = probs_tft_clean
                else:
                    # Prendre les derni√®res probabilit√©s
                    probs_subset = probs_tft_clean[-len(y_test):]
                    valid_mask = ~(np.isnan(probs_subset) | np.isnan(y_test))
                    if valid_mask.sum() > 0:
                        auc_tft = roc_auc_score(y_test[valid_mask], probs_subset[valid_mask])
                    else:
                        auc_tft = 0.5
                    y_test_aligned = y_test
                    probs_tft_aligned = probs_subset
                
                # Nettoyer les probabilit√©s align√©es avant de calculer les pr√©dictions
                probs_tft_aligned = np.nan_to_num(probs_tft_aligned, nan=0.5, posinf=0.99, neginf=0.01)
                probs_tft_aligned = np.clip(probs_tft_aligned, 0.0, 1.0)
                
                # Calculer les pr√©dictions avec le seuil optimal
                pred_tft_optimal = (probs_tft_aligned >= thresh_tft).astype(int)
                precision_tft = precision_score(y_test_aligned, pred_tft_optimal, zero_division=0)
                f1_tft = f1_score(y_test_aligned, pred_tft_optimal)
            else:
                auc_tft = 0.0
                precision_tft = 0.0
                f1_tft = 0.0
            
            print(f"Seuil optimal : {thresh_tft:.2f}")
            print(f"Capital final : {capital_tft:.2f}‚Ç¨")
            print(f"PnL : {pnl_tft:+.2f}‚Ç¨")
            print(f"ROI annualized : {roi_annualized_tft:.2f}%")
            print(f"AUC : {auc_tft:.4f}")
            print(f"Precision : {precision_tft:.4f}")
            print(f"F1-score : {f1_tft:.4f}")
            print_confusion_matrix(y_test_aligned, pred_tft_optimal, "TFT")
            print(f"Trades/jour moyen : {avg_trades_tft:.4f}")
            print(f"vs Buy&Hold : {pnl_tft - pnl_bh:+.2f}‚Ç¨")
            
            # Optionnel: Extraire la feature importance
            # Note: Pour cela, il faudrait modifier train_tft pour retourner aussi le mod√®le
            # Pour l'instant, cette fonctionnalit√© est disponible via extract_tft_feature_importance()
            # si vous avez acc√®s au mod√®le TFT entra√Æn√© et au dataset
            try:
                # Exemple d'utilisation (n√©cessite que train_tft retourne aussi le mod√®le)
                # tft_model, training_dataset = ...  # R√©cup√©rer depuis train_tft
                # importance_results = extract_tft_feature_importance(
                #     tft_model, training_dataset, feature_cols, top_k=20
                # )
                # print_importance_summary(importance_results)
                pass
            except Exception as e:
                # Silencieux si la feature importance n'est pas disponible
                pass
        else:
            capital_tft = 0
            pnl_tft = -1000
            roi_annualized_tft = -100.0
            avg_trades_tft = 0.0
            # Le message d'erreur a d√©j√† √©t√© affich√© dans train_tft
            # Ne pas afficher un message g√©n√©rique ici
    else:
        capital_tft = 0
        pnl_tft = -1000
        roi_annualized_tft = -100.0
        avg_trades_tft = 0.0

    # ========== R√âSUM√â FINAL ==========
    print("\n" + "="*80)
    print("üèÜ TABLEAU COMPARATIF FINAL")
    print("="*80)

    results_list = [
        {
            "Strat√©gie": "Buy & Hold",
            "Capital": capital_bh,
            "PnL": pnl_bh,
            "vs B&H": 0.00,
            "ROI annualized %": f"{roi_annualized_bh:.2f}%",
            "Trades/jour": f"{avg_trades_bh:.4f}"
        },
        {
            "Strat√©gie": "RF Baseline (22 feat)",
            "Capital": capital_basic,
            "PnL": pnl_basic,
            "vs B&H": pnl_basic - pnl_bh,
            "ROI annualized %": f"{roi_annualized_basic:.2f}%",
            "Trades/jour": f"{avg_trades_basic:.4f}"
        },
        {
            "Strat√©gie": "RF Advanced (74 feat)",
            "Capital": capital_adv,
            "PnL": best_pnl_adv,
            "vs B&H": best_pnl_adv - pnl_bh,
            "ROI annualized %": f"{roi_annualized_adv:.2f}%",
            "Trades/jour": f"{avg_trades_adv:.4f}"
        },
        {
            "Strat√©gie": "Logistic Regression",
            "Capital": capital_lr,
            "PnL": best_pnl_lr,
            "vs B&H": best_pnl_lr - pnl_bh,
            "ROI annualized %": f"{roi_annualized_lr:.2f}%",
            "Trades/jour": f"{avg_trades_lr:.4f}"
        }
    ]

    if signals_mr is not None:
        results_list.append({
            "Strat√©gie": "MiniRocket (TS)",
            "Capital": capital_mr,
            "PnL": pnl_mr,
            "vs B&H": pnl_mr - pnl_bh,
            "ROI annualized %": f"{roi_annualized_mr:.2f}%",
            "Trades/jour": f"{avg_trades_mr:.4f}"
        })
    else:
        avg_trades_mr = 0.0
    
    if capital_tab is not None:
        results_list.append({
            "Strat√©gie": "TabNet (Optimized)",
            "Capital": capital_tab,
            "PnL": pnl_tab,
            "vs B&H": pnl_tab - pnl_bh,
            "ROI annualized %": f"{roi_annualized_tab:.2f}%",
            "Trades/jour": f"{avg_trades_tab:.4f}"
        })
    
    if capital_tft is not None:
        results_list.append({
            "Strat√©gie": "TemporalFusionTransformer",
            "Capital": capital_tft,
            "PnL": pnl_tft,
            "vs B&H": pnl_tft - pnl_bh,
            "ROI annualized %": f"{roi_annualized_tft:.2f}%",
            "Trades/jour": f"{avg_trades_tft:.4f}"
        })

    results = pd.DataFrame(results_list)

    print(results.to_string(index=False))

    # Analyse
    print("\n" + "="*80)
    print("üìà ANALYSE")
    print("="*80)

    winner = results.loc[results["PnL"].idxmax()]
    print(f"Meilleure strat√©gie : {winner['Strat√©gie']}")
    print(f"PnL : {winner['PnL']:.2f}‚Ç¨")

    ml_pnls = [best_pnl_adv, pnl_basic, best_pnl_lr]
    if signals_mr is not None:
        ml_pnls.append(pnl_mr)
    if capital_tab is not None:
        ml_pnls.append(pnl_tab)
    ml_best_pnl = max(ml_pnls)

    if pnl_bh > ml_best_pnl:
        print("\n‚ö†Ô∏è  Buy & Hold BAT tous les mod√®les ML !")
        print("   ‚Üí Les signaux ML n'ajoutent pas de valeur")
        print("   ‚Üí Mieux vaut garder le Bitcoin")
    else:
        print("\n‚úÖ Un mod√®le ML bat Buy & Hold !")
        print(f"   ‚Üí Gain vs B&H : {ml_best_pnl - pnl_bh:.2f}‚Ç¨")

    print("="*80)
    
    # Sauvegarder les r√©sultats de la session
    session_params = {
        "timeframe": TIMEFRAME,
        "start_date": str(start_dt.date()),
        "end_date": str(end_dt.date()),
        "horizon": HORIZON_STEPS,
        "fee_roundtrip": FEE_ROUNDTRIP,
        "train_ratio": TRAIN_RATIO,
        "valid_ratio": VALID_RATIO,
        "lookback": LOOKBACK,
        "num_kernels": NUM_KERNELS,
        "random_seed": RANDOM_SEED,
        "test_period_start": str(test_df['Timestamp'].iloc[0]) if 'Timestamp' in test_df.columns else "N/A",
        "test_period_end": str(test_df['Timestamp'].iloc[-1]) if 'Timestamp' in test_df.columns else "N/A",
        "test_samples": len(test_df),
        "train_samples": len(train_df),
        "valid_samples": len(valid_df),
        "models_config_file": args.models_config,
        "enabled_models": ", ".join(enabled_models)
    }
    
    save_session_results(results, session_params)


if __name__ == "__main__":
    main()
