"""
This script replicates the core workflow from the P09 Jupyter notebook.  It
fetches or loads Bitcoin OHLCV data, engineers technical indicators,
constructs a binary target indicating when to buy, trains a RandomForest
classifier, optionally trains a MiniROCKET-based classifier if the
necessary library is available, evaluates each model on a hold‑out test
set, and runs a simple backtest over the signals generated by each
classifier.

The script is designed to be run from the command line.  It expects
either a CSV file containing historical OHLCV data (with columns
``Timestamp``, ``Open``, ``High``, ``Low``, ``Close``, ``Volume``) or
will attempt to fetch data from Binance via the `ccxt` library if it is
installed and no CSV is provided.  See the ``main`` function for
details.

Requirements
------------

* pandas, numpy, scikit‑learn
* ta (for technical indicators)
* If you wish to use MiniROCKET: sktime>=0.9.0
* Optional: ccxt to fetch live data from Binance

Because this environment may not have internet access or the required
libraries installed, parts of the script that rely on external
dependencies are wrapped in try/except blocks.  If a library is
unavailable, the associated functionality will be skipped and a
warning printed instead.

Example usage::

    python trading_script.py --start 2021-01-01 --end 2024-12-31 \
        --lookback 512 --horizon 24 --timeframe 1h --pct-capital 0.1 \
        --csv-file btc_data.csv

This will load data from ``btc_data.csv`` and run the analysis.
"""

import argparse
import datetime as _dt
import os
import sys
from typing import Optional, Tuple

import numpy as np
import pandas as pd

try:
    import ta  # type: ignore
except ImportError as e:
    raise ImportError(
        "The 'ta' library is required for technical indicators. "
        "Install with `pip install ta`.") from e

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import RidgeClassifierCV
    from sklearn.metrics import f1_score
    from sklearn.preprocessing import StandardScaler
except ImportError:
    raise ImportError(
        "scikit‑learn is required. Install with `pip install scikit‑learn`."
    )

# Import centralized utilities
from backtest import Backtest
from utils import (
    fetch_ohlcv_binance,
    compute_features,
    construct_target,
    split_data,
    evaluate_metrics,
)

# All functions (fetch_ohlcv_binance, compute_features, construct_target,
# split_data, evaluate_metrics) and Backtest class are now imported from
# utils.py and backtest.py


def try_train_minirocket(
    df: pd.DataFrame,
    lookback: int,
    y_train: np.ndarray,
    y_valid: np.ndarray,
    y_test: np.ndarray,
    train_start_idx: int,
    train_end_idx: int,
    valid_end_idx: int,
    fee_roundtrip: float,
    pct_capital: float,
    test_df: pd.DataFrame,
) -> Tuple[Optional[dict], Optional[float], Optional[float]]:
    """Attempt to train and evaluate a MiniROCKET classifier.

    This function uses sktime's MiniRocket to transform multivariate time series
    (OHLCV data) and trains a RandomForest classifier on the resulting features.
    If sktime is not installed, it returns (None, None, None).
    
    Returns
    -------
    metrics : dict or None
        Classification metrics.
    pnl : float or None
        Profit and loss.
    roi_annualized : float or None
        Annualized ROI percentage.
    avg_trades : float or None
        Average number of trades per day.
    """
    try:
        from sktime.transformations.panel.rocket import MiniRocket
    except ImportError:
        print(
            "MiniROCKET requires sktime. Please install sktime>=0.9.0 to use this model."
        )
        return None, None, None

    # Prepare multivariate time series from raw OHLCV data
    # Use Close, Volume, and returns as channels for better pattern detection
    df_ts = df.copy()
    
    # Normalize each channel independently (z-score normalization)
    # Channel 1: Close price (normalized)
    close_mean = df_ts["Close"].mean()
    close_std = df_ts["Close"].std()
    close_norm = (df_ts["Close"] - close_mean) / (close_std + 1e-8)
    
    # Channel 2: Volume (normalized)
    volume_mean = df_ts["Volume"].mean()
    volume_std = df_ts["Volume"].std()
    volume_norm = (df_ts["Volume"] - volume_mean) / (volume_std + 1e-8)
    
    # Channel 3: Returns (log returns)
    returns = np.log(df_ts["Close"] / df_ts["Close"].shift(1)).fillna(0)
    
    # Channel 4: High-Low spread (normalized)
    hl_spread = (df_ts["High"] - df_ts["Low"]) / df_ts["Close"]
    hl_mean = hl_spread.mean()
    hl_std = hl_spread.std()
    hl_norm = (hl_spread - hl_mean) / (hl_std + 1e-8)
    
    # Build sliding windows for time series
    def create_sequences(data, lookback_window):
        """Create sequences of length lookback_window from data."""
        sequences = []
        for i in range(len(data) - lookback_window + 1):
            sequences.append(data[i:i+lookback_window])
        return np.array(sequences)
    
    # Create sequences for each channel
    close_seqs = create_sequences(close_norm.values, lookback)
    volume_seqs = create_sequences(volume_norm.values, lookback)
    returns_seqs = create_sequences(returns.values, lookback)
    hl_seqs = create_sequences(hl_norm.values, lookback)
    
    # Stack channels: (n_samples, n_channels, series_length)
    # Each sequence i uses data from index i to i+lookback-1
    # The target for sequence i should be at index i+lookback-1 (predicting future from past)
    n_samples = len(df_ts) - lookback + 1
    X_ts = np.zeros((n_samples, 4, lookback))
    X_ts[:, 0, :] = close_seqs
    X_ts[:, 1, :] = volume_seqs
    X_ts[:, 2, :] = returns_seqs
    X_ts[:, 3, :] = hl_seqs
    
    # Reconstruct full target array and align with sequences
    # Sequence at index i predicts target at index i+lookback-1
    y_full = np.concatenate([y_train, y_valid, y_test])
    
    # Calculate split points for sequences
    # Original data: [0...train_end_idx-1][train_end_idx...valid_end_idx-1][valid_end_idx...end]
    # Sequences: sequence i uses data[i:i+lookback], predicts target[i+lookback-1]
    # Training sequences: i from 0 to train_end_idx-lookback (predicts lookback-1 to train_end_idx-1)
    # Validation sequences: i from train_end_idx-lookback+1 to valid_end_idx-lookback (predicts train_end_idx to valid_end_idx-1)
    # Test sequences: i from valid_end_idx-lookback+1 to end (predicts valid_end_idx to end)
    
    train_end_seq = max(0, train_end_idx - lookback + 1)
    valid_end_seq = max(0, valid_end_idx - lookback + 1)
    
    # Ensure we have enough data
    if train_end_seq <= 0 or valid_end_seq <= train_end_seq or n_samples <= valid_end_seq:
        print(f"Warning: Not enough data for MiniROCKET with lookback={lookback}. Skipping.")
        return None, None, None

    X_train_ts = X_ts[:train_end_seq]
    X_valid_ts = X_ts[train_end_seq:valid_end_seq]
    X_test_ts = X_ts[valid_end_seq:]
    
    # Align targets: sequence i predicts target[i+lookback-1]
    y_train_seq = y_full[lookback-1:lookback-1+len(X_train_ts)]
    y_valid_seq = y_full[lookback-1+len(X_train_ts):lookback-1+len(X_train_ts)+len(X_valid_ts)]
    y_test_seq = y_full[lookback-1+len(X_train_ts)+len(X_valid_ts):lookback-1+len(X_train_ts)+len(X_valid_ts)+len(X_test_ts)]
    
    # Final check
    if len(y_train_seq) != len(X_train_ts) or len(y_valid_seq) != len(X_valid_ts) or len(y_test_seq) != len(X_test_ts):
        print(f"Warning: Target alignment failed. Skipping MiniROCKET.")
        return None, None, None

    # Fit MiniROCKET with more kernels for better feature extraction
    rocket = MiniRocket(num_kernels=10000, random_state=42)
    rocket.fit(X_train_ts)
    X_train_transform = rocket.transform(X_train_ts)
    X_valid_transform = rocket.transform(X_valid_ts)
    X_test_transform = rocket.transform(X_test_ts)

    # Standardize features
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train_transform)
    X_valid_std = scaler.transform(X_valid_transform)
    X_test_std = scaler.transform(X_test_transform)

    # Use RandomForest instead of RidgeClassifier for better performance
    # Optimize on validation set
    best_f1 = 0
    best_clf = None
    best_threshold = 0.5
    
    # Try different class weights and thresholds
    for class_weight in [None, "balanced", {0: 1, 1: 2}, {0: 1, 1: 3}]:
        clf = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_leaf=5,
            class_weight=class_weight,
            random_state=42,
            n_jobs=-1,
        )
        clf.fit(X_train_std, y_train_seq)
        
        # Get probabilities
        y_valid_prob = clf.predict_proba(X_valid_std)[:, 1]
        
        # Try different thresholds
        for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
            y_valid_pred = (y_valid_prob >= threshold).astype(int)
            f1 = f1_score(y_valid_seq, y_valid_pred)
            if f1 > best_f1:
                best_f1 = f1
                best_clf = clf
                best_threshold = threshold
    
    # Use best model and threshold
    y_test_prob = best_clf.predict_proba(X_test_std)[:, 1]
    y_pred = (y_test_prob >= best_threshold).astype(int)
    y_prob = y_test_prob

    metrics = evaluate_metrics(y_test_seq, y_pred, y_prob)

    # Backtest - align signals with test_df
    # test_df starts at valid_end_idx in original df
    # Our test sequences start at valid_end_seq = valid_end_idx - lookback + 1
    # So we need to pad the beginning with zeros (can't predict without lookback window)
    # or use the predictions starting from the point where we have sequences
    signals_for_backtest = np.zeros(len(test_df))
    # The first lookback-1 rows of test_df don't have predictions (need history)
    # Start predictions from index lookback-1 in test_df
    start_idx = lookback - 1
    if start_idx < len(test_df) and len(y_pred) > 0:
        end_idx = min(start_idx + len(y_pred), len(test_df))
        signals_for_backtest[start_idx:end_idx] = y_pred[:end_idx-start_idx]
    
    backtester = Backtest(
        df_bt=test_df.reset_index(drop=True),
        signals=signals_for_backtest,
        fee_roundtrip=fee_roundtrip,
        pct_capital=pct_capital,
        capital_init=1000.0,
    )
    capital = backtester.run()
    pnl = capital - 1000.0
    roi_annualized = backtester.get_roi_annualized()
    avg_trades = backtester.get_avg_trades_per_day()

    return metrics, pnl, roi_annualized, avg_trades


def main() -> None:
    parser = argparse.ArgumentParser(description="Train trading models on BTC data.")
    parser.add_argument(
        "--pair",
        default="BTC/USDC",
        help="Trading pair (default: BTC/USDC)",
    )
    parser.add_argument(
        "--timeframe",
        default="1h",
        help="Candle timeframe (e.g. 1h, 15m)",
    )
    parser.add_argument(
        "--start",
        type=str,
        default="2021-01-01",
        help="Start date in YYYY-MM-DD format",
    )
    parser.add_argument(
        "--end",
        type=str,
        default="2025-01-01",
        help="End date in YYYY-MM-DD format",
    )
    parser.add_argument(
        "--lookback",
        type=int,
        default=512,
        help="Context length for sequence models (unused here)",
    )
    parser.add_argument(
        "--horizon",
        type=int,
        default=24,
        help="Horizon in steps for the ROI and backtest",
    )
    parser.add_argument(
        "--fee",
        type=float,
        default=0.002,
        help="Roundtrip fee (e.g. 0.002 = 0.2%)",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=None,
        help="Threshold for ROI to label a Buy (defaults to fee)",
    )
    parser.add_argument(
        "--pct-capital",
        type=float,
        default=0.1,
        help="Fraction of capital to invest per trade",
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.8,
        help="Fraction of data to use for training",
    )
    parser.add_argument(
        "--valid-ratio",
        type=float,
        default=0.1,
        help="Fraction of data to use for validation",
    )
    parser.add_argument(
        "--csv-file",
        type=str,
        default=None,
        help="Path to a CSV file containing OHLCV data.  If omitted and ccxt is "
        "installed, data will be fetched from Binance.",
    )

    args = parser.parse_args()

    start_dt = _dt.datetime.fromisoformat(args.start)
    end_dt = _dt.datetime.fromisoformat(args.end)

    # Load or fetch data
    if args.csv_file is not None and os.path.isfile(args.csv_file):
        df = pd.read_csv(args.csv_file, parse_dates=["Timestamp"])
        required_cols = {"Timestamp", "Open", "High", "Low", "Close", "Volume"}
        if not required_cols.issubset(df.columns):
            raise ValueError(
                f"CSV file must contain columns: {required_cols}. Found: {df.columns}"
            )
    else:
        # Attempt to fetch from Binance
        print("Fetching data from Binance via ccxt...")
        df = fetch_ohlcv_binance(
            pair=args.pair,
            timeframe=args.timeframe,
            start=start_dt,
            end=end_dt,
        )

    # Compute features
    df = compute_features(df)

    # Determine threshold if not provided: default threshold equals fee
    thresh = args.threshold if args.threshold is not None else args.fee

    # Construct target
    target = construct_target(df, horizon=args.horizon, threshold=thresh)

    # Define feature columns
    feature_cols = [
        "logret_1", "logret_5", "logret_20", "vol_20", "vol_50",
        "ma_diff", "ema_diff", "rsi14", "macd", "macd_signal",
        "atr14", "adx14",
    ]

    # Drop rows corresponding to NaN in target (last horizon steps)
    df = df.iloc[: len(target)]
    target = target.iloc[: len(df)]

    # Split data chronologically into train/valid/test
    X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(
        df, feature_cols, target, args.train_ratio, args.valid_ratio
    )

    # Train RandomForest
    rf = RandomForestClassifier(
        n_estimators=800,
        max_depth=20,
        min_samples_leaf=10,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1,
    )
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    y_prob_rf = rf.predict_proba(X_test)[:, 1]
    metrics_rf = evaluate_metrics(y_test, y_pred_rf, y_prob_rf)

    # Backtest RandomForest
    # Create test DataFrame with Close column for backtest
    n = len(df)
    train_end = int(n * args.train_ratio)
    valid_end = train_end + int(n * args.valid_ratio)
    test_start_idx = valid_end
    test_df = df.iloc[test_start_idx:].copy().reset_index(drop=True)
    backtester_rf = Backtest(
        df_bt=test_df,
        signals=y_pred_rf,
        fee_roundtrip=args.fee,
        pct_capital=args.pct_capital,
        capital_init=1000.0,
    )
    capital_rf = backtester_rf.run()
    pnl_rf = capital_rf - 1000.0
    roi_annualized_rf = backtester_rf.get_roi_annualized()

    # Attempt MiniROCKET model
    result_mr = try_train_minirocket(
        df=df,
        lookback=args.lookback,
        y_train=y_train,
        y_valid=y_valid,
        y_test=y_test,
        train_start_idx=0,
        train_end_idx=train_end,
        valid_end_idx=valid_end,
        fee_roundtrip=args.fee,
        pct_capital=args.pct_capital,
        test_df=test_df,
    )
    if result_mr[0] is not None:
        metrics_mr, pnl_mr, roi_annualized_mr, avg_trades_mr = result_mr
    else:
        metrics_mr, pnl_mr, roi_annualized_mr, avg_trades_mr = None, None, None, None

    # Report results
    print("\n=== Model Comparison ===")
    print("RandomForest metrics:")
    for k, v in metrics_rf.items():
        print(f"  {k}: {v:.4f}" if v is not None else f"  {k}: N/A")
    print(f"  PnL (backtest): {pnl_rf:.2f}")
    print(f"  ROI annualized: {roi_annualized_rf:.2f}%")
    avg_trades_rf = backtester_rf.get_avg_trades_per_day()
    print(f"  Trades/jour moyen: {avg_trades_rf:.4f}")

    if metrics_mr is not None:
        print("\nMiniROCKET metrics:")
        for k, v in metrics_mr.items():
            print(f"  {k}: {v:.4f}" if v is not None else f"  {k}: N/A")
        print(f"  PnL (backtest): {pnl_mr:.2f}")
        print(f"  ROI annualized: {roi_annualized_mr:.2f}%")
        print(f"  Trades/jour moyen: {avg_trades_mr:.4f}")
    else:
        print("\nMiniROCKET model could not be evaluated. Ensure sktime is installed.")


if __name__ == "__main__":
    main()