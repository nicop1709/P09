{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a545190",
   "metadata": {},
   "source": [
    "# Projet 9 — Signaux de trading BTC–USDC : RandomForest vs TabNet\n",
    "\n",
    "Ce notebook suit le **plan prévisionnel** :  \n",
    "- Données Binance (BTC–USDC)  \n",
    "- Cible réaliste basée sur un ROI futur et frais  \n",
    "- Baseline **RandomForestClassifier**  \n",
    "- Modèle récent **TabNet** (IBM TSFM) utilisé comme **forecaster**, puis conversion en signal de trading  \n",
    "- Essai Modèle récent **Tiny Time Mixer (TTM)** (IBM TSFM) utilisé comme **forecaster**, puis conversion en signal de trading mais trop long quand beaucoup de donnée. Code commenté\n",
    "- Évaluation **ML + trading** (PnL, drawdown)\n",
    "\n",
    "> Remarque : pour garder une comparaison simple et robuste, TTM est utilisé ici en *forecasting* du `Close` puis transformé en signal `Buy/No-trade` via un seuil de ROI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850b88c",
   "metadata": {},
   "source": [
    "## 0) Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "71b58cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres globaux (à ajuster si besoin)\n",
    "PAIR = \"BTC/USDC\"\n",
    "TIMEFRAME = \"1h\"          # \"15m\" ou \"1h\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-12-15\"\n",
    "CAPITAL_INIT = 1000\n",
    "# Fenêtre et horizon\n",
    "LOOKBACK = 512            # contexte pour TTM\n",
    "HORIZON_STEPS = 24        # 24h si TIMEFRAME = 1h ; 16 si 4h en 15m ; etc.\n",
    "\n",
    "# Trading/label\n",
    "FEE_ROUNDTRIP = 0.002     # ~0.20% frais achat+vente (spot, simplifié)\n",
    "THRESH = FEE_ROUNDTRIP    # seuil ROI minimal pour décider \"Buy\"\n",
    "\n",
    "# Split temporel (chronologique)\n",
    "TRAIN_RATIO = 0.80\n",
    "VALID_RATIO = 0.10        # test = reste\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d5743",
   "metadata": {},
   "source": [
    "## 1) Installations & imports & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0498777",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd9e38",
   "metadata": {},
   "source": [
    "#### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5dd4a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout du répertoire racine au sys.path pour s'assurer que le module utils est trouvé\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Obtenir le répertoire de travail actuel (où se trouve le notebook)\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Forcer le rechargement du module utils pour éviter les problèmes de cache du kernel Jupyter\n",
    "if 'utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# Import des classes depuis utils\n",
    "from backtest import Backtest\n",
    "from utils import plot_backtest, prepare_data_advanced_features ,prepare_data_min_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "553021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Si vous êtes sur Kaggle / Colab : décommentez selon l'environnement.\n",
    "\n",
    "# !pip -q install ccxt ta scikit-learn matplotlib pandas numpy torch transformers accelerate\n",
    "\n",
    "# --- TSFM (IBM) : on reproduit l'approche du notebook Kaggle TTM ---\n",
    "# Le notebook Kaggle clone IBM/tsfm et importe tsfm_public.\n",
    "# Ici on fait pareil pour être aligné.\n",
    "import os, sys, subprocess, textwrap\n",
    "\n",
    "TSFM_DIR = \"tsfm\"\n",
    "# if not os.path.isdir(TSFM_DIR):\n",
    "#     subprocess.check_call([\"bash\",\"-lc\", f\"git clone --depth 1 --branch v0.2.9 https://github.com/IBM/tsfm.git {TSFM_DIR}\"])\n",
    "\n",
    "# # Use sys.executable to ensure we install in the same Python environment as the notebook kernel\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"-e\", TSFM_DIR])\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import ta\n",
    "\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d7f91",
   "metadata": {},
   "source": [
    "### 1.1 Functions utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6294e5",
   "metadata": {},
   "source": [
    "#### Backtest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0929bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trader:\n",
    "#     def __init__(self, row: pd.Series, idx: int, idx_entry: int, signal: np.ndarray, capital: float, portfolio: float, position: float, qty: float, entry_price: float, exit_price: float, fee_roundtrip=0.002, pct_capital=1, debug=False, trade_list=[]):\n",
    "#         self.row = row\n",
    "#         self.idx = idx\n",
    "#         self.signal = signal\n",
    "#         self.fee_roundtrip = fee_roundtrip\n",
    "#         self.pct_capital = pct_capital\n",
    "#         self.capital = capital    \n",
    "#         self.portfolio = portfolio\n",
    "#         self.position = position\n",
    "#         self.qty = qty\n",
    "#         self.entry_price = entry_price\n",
    "#         self.exit_price = exit_price\n",
    "#         self.debug = debug\n",
    "#         self.idx_entry = idx_entry\n",
    "#         self.trade_list = trade_list\n",
    "#         self.timestamp_entry = None\n",
    "#         self.max_drawdown_pct = 0\n",
    "\n",
    "\n",
    "#     def _buy(self):\n",
    "#         self.qty = self.pct_capital * self.capital / self.row[\"Close\"]\n",
    "#         position_value = self.qty * self.row[\"Close\"]\n",
    "#         self.position = position_value  # Montant investi dans la position\n",
    "#         self.entry_price = self.row[\"Close\"]\n",
    "#         buy_fees = self.fee_roundtrip * position_value / 2\n",
    "#         self.capital -= (position_value + buy_fees)\n",
    "#         self.portfolio = position_value  # Portfolio = valeur de la position\n",
    "#         self.idx_entry = self.idx\n",
    "#         self.timestamp_entry = self.row[\"Timestamp\"]\n",
    "#         if self.debug:\n",
    "#             print(f\"Idx: {self.idx} / Buy: {self.qty:.8f} @ {self.entry_price:.2f}\")\n",
    "#         return True\n",
    "\n",
    "#     def _sell(self):\n",
    "#         sell_value = self.qty * self.row[\"Close\"]\n",
    "#         sell_fees = self.fee_roundtrip * sell_value / 2\n",
    "#         PnL = self.qty * (self.row[\"Close\"] - self.entry_price)\n",
    "#         PnL_net = PnL - sell_fees\n",
    "#         capital_before_sell = self.portfolio+self.capital\n",
    "#         self.capital += sell_value - sell_fees\n",
    "#         self.position = 0  # Plus de position ouverte\n",
    "#         self.exit_price = self.row[\"Close\"]\n",
    "#         self.portfolio = 0  # Portfolio vide après vente\n",
    "#         self.max_drawdown_pct = (PnL/capital_before_sell)*100\n",
    "#         self.trade_list.append({\n",
    "#             \"idx\": self.idx,\n",
    "#             \"idx_entry\": self.idx_entry,\n",
    "#             \"Timestamp\": self.row[\"Timestamp\"],\n",
    "#             \"Timestamp_entry\": self.timestamp_entry,\n",
    "#             \"qty\": self.qty,\n",
    "#             \"entry_price\": self.entry_price,\n",
    "#             \"exit_price\": self.exit_price,\n",
    "#             \"PnL\": PnL,\n",
    "#             \"PnL_net\": PnL_net,\n",
    "#             \"Capital\": self.capital,\n",
    "#             \"MaxDrawDown\": self.max_drawdown_pct,\n",
    "#         })\n",
    "\n",
    "#         if self.debug:\n",
    "#             print(f\"Idx: {self.idx} / Sell: {self.qty:.8f} @ {self.exit_price:.2f}\")\n",
    "#             print(f\"PnL: {PnL:.2f}\")\n",
    "#             print(f\"PnL net (après frais): {PnL_net:.2f}\")\n",
    "#             print(f\"Portfolio: {self.portfolio:.2f}\")\n",
    "#             print(f\"Capital: {self.capital:.2f}\")\n",
    "#         return True\n",
    "\n",
    "#     def run(self):\n",
    "#         # Conversion du signal en int (gère les cas numpy array et scalar)\n",
    "#         sig = int(self.signal) if isinstance(self.signal, (np.ndarray, np.generic)) else int(self.signal)\n",
    "        \n",
    "#         # Mise à jour du portfolio si position ouverte (valeur actuelle de la position)\n",
    "#         if self.position > 0:\n",
    "#             self.portfolio = self.qty * self.row[\"Close\"]\n",
    "        \n",
    "#         if self.debug:\n",
    "#             print(f\"Idx: {self.idx} / Signal: {sig} / Position: {self.position:.2f} / Portfolio: {self.portfolio:.2f}\")\n",
    "        \n",
    "#         # Achat : signal=1 et pas de position ouverte\n",
    "#         if sig == 1 and self.position == 0:\n",
    "#             self._buy()\n",
    "#         # Vente : signal=0 et position ouverte (on vend dès que le signal passe à 0)\n",
    "#         elif sig == 0 and self.position > 0 and self.idx >= self.idx_entry + HORIZON_STEPS:\n",
    "#             self._sell()\n",
    "        \n",
    "#         return self.portfolio, self.capital, self.position, self.qty, self.entry_price, self.exit_price, self.trade_list     \n",
    "\n",
    "\n",
    "# class Backtest:\n",
    "#     def __init__(self, df_bt: pd.DataFrame, signal: np.ndarray, fee_roundtrip=0.002, pct_capital=1, capital_init=1000, debug=False):\n",
    "#         self.df_bt = df_bt\n",
    "#         self.signal = signal\n",
    "#         self.fee_roundtrip = fee_roundtrip\n",
    "#         self.pct_capital = pct_capital\n",
    "#         self.capital_init = capital_init  # Sauvegarder le capital initial\n",
    "#         self.capital = capital_init\n",
    "#         self.position = 0\n",
    "#         self.qty = 0\n",
    "#         self.entry_price = 0\n",
    "#         self.exit_price = 0\n",
    "#         self.portfolio = 0\n",
    "#         self.debug = debug\n",
    "#         self.idx_entry = 0\n",
    "#         self.trade_list = []\n",
    "#         self.max_drawdown_pct = 0\n",
    "#         self.run()\n",
    "#         self.print_stats()\n",
    "\n",
    "#     def run(self):\n",
    "#         trader = Trader([], 0, 0, 0, self.capital, self.portfolio, self.position, self.qty, self.entry_price, self.exit_price, self.fee_roundtrip, self.pct_capital, debug=self.debug, trade_list=self.trade_list)\n",
    "#         last_idx = None\n",
    "#         for i, row in self.df_bt.iterrows():\n",
    "#             trader.row = row\n",
    "#             trader.idx = i\n",
    "#             trader.signal = self.signal[i]\n",
    "#             trader.run()\n",
    "#             self.portfolio = trader.portfolio\n",
    "#             self.capital = trader.capital   \n",
    "#             self.position = trader.position\n",
    "#             self.qty = trader.qty\n",
    "#             self.entry_price = trader.entry_price\n",
    "#             self.exit_price = trader.exit_price\n",
    "#             self.idx_entry = trader.idx_entry\n",
    "#             self.timestamp_entry = trader.timestamp_entry\n",
    "#             last_idx = i\n",
    "\n",
    "#         # Clôture forcée si position ouverte en fin de backtest\n",
    "#         if self.position > 0 and last_idx is not None:\n",
    "#             last_row = self.df_bt.iloc[last_idx]\n",
    "#             sell_value = self.qty * last_row[\"Close\"]\n",
    "#             sell_fees = self.fee_roundtrip * sell_value / 2\n",
    "#             PnL = self.qty * (last_row[\"Close\"] - self.entry_price)\n",
    "#             PnL_net = PnL - sell_fees\n",
    "#             self.capital += sell_value - sell_fees\n",
    "#             self.portfolio = 0\n",
    "#             self.position = 0\n",
    "#             self.trade_list.append({\n",
    "#                 \"idx\": last_idx,\n",
    "#                 \"idx_entry\": self.idx_entry,\n",
    "#                 \"Timestamp\": last_row[\"Timestamp\"],\n",
    "#                 \"Timestamp_entry\": self.timestamp_entry,\n",
    "#                 \"qty\": self.qty,\n",
    "#                 \"entry_price\": self.entry_price,\n",
    "#                 \"exit_price\": last_row[\"Close\"],\n",
    "#                 \"PnL\": PnL,\n",
    "#                 \"PnL_net\": PnL_net,\n",
    "#                 \"Capital\": self.capital,\n",
    "#                 \"MaxDrawDown\": self.max_drawdown_pct,\n",
    "#             })\n",
    "#             self.qty = 0\n",
    "#             self.entry_price = 0\n",
    "#             self.exit_price = 0\n",
    "        \n",
    "#         days = (self.df_bt.iloc[-1][\"Timestamp\"] - self.df_bt.iloc[0][\"Timestamp\"]).days\n",
    "#         if days <= 0:\n",
    "#             days = 1  # Avoid division by zero\n",
    "#         self.days = days\n",
    "#         self.PnL = self.capital - self.capital_init\n",
    "#         self.ROI_pct = self.PnL / self.capital_init *100\n",
    "#         self.ROI_day_pct = self.PnL / self.capital_init / days * 100\n",
    "#         # Calculate annualized ROI: convert ROI_pct from percentage to decimal first\n",
    "#         roi_decimal = self.ROI_pct / 100\n",
    "#         if roi_decimal <= -1:\n",
    "#             # If we lost more than 100%, return -100%\n",
    "#             self.ROI_annualized_pct = -100.0\n",
    "#         else:\n",
    "#             self.ROI_annualized_pct = ((1 + roi_decimal) ** (365.0 / days) - 1) * 100\n",
    "#         self.df_trades = pd.DataFrame(self.trade_list)\n",
    "#         self.win_rates = self.df_trades[\"PnL\"].apply(lambda x: x > 0).mean()*100\n",
    "#         self.nb_trades = len(self.df_trades)\n",
    "#         self.nb_trades_by_day = self.nb_trades / days\n",
    "#         self.max_drawdown_pct = self.df_trades[\"MaxDrawDown\"].max()\n",
    "#         return self.portfolio, self.capital, self.position, self.qty, self.entry_price, self.exit_price, self.trade_list\n",
    "    \n",
    "#     def print_stats(self):\n",
    "#         print(f\"Days: {self.days}\")\n",
    "#         print(f\"Portfolio: {self.portfolio}\")\n",
    "#         print(f\"Capital: {self.capital}\")\n",
    "#         print(f\"PnL: {self.capital - CAPITAL_INIT}\")\n",
    "#         print(f\"Position: {self.position}\")\n",
    "#         print(f\"ROI: {self.ROI_pct:.2f}%\")\n",
    "#         print(f\"ROI annualized: {self.ROI_annualized_pct:.2f}%\")\n",
    "#         print(f\"ROI day: {self.ROI_day_pct:.2f}%\")\n",
    "#         print(f\"Win rate: {self.win_rates:.2f}%\")\n",
    "#         print(f\"Nb trades: {self.nb_trades}\")\n",
    "#         print(f\"Nb trades par jour: {self.nb_trades_by_day:.2f}\")\n",
    "#         print(f\"Max DrawDown: {self.max_drawdown_pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2383e1",
   "metadata": {},
   "source": [
    "#### Plot backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5feb5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objs as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# def plot_backtest(backtester):\n",
    "#     # On suppose que trades_df == backtester.df_trades déjà généré avec l'algo ci-dessus\n",
    "#     trades_df = backtester.df_trades\n",
    "\n",
    "#     # Pour le graphique, récupérer le temps et close price\n",
    "#     df_curves = backtester.df_bt.reset_index(drop=True)\n",
    "#     df_curves[\"Timestamp_entry\"] = df_curves[\"Timestamp\"]\n",
    "#     df_curves = pd.merge(df_curves, trades_df[[\"Timestamp\", \"exit_price\",\"Capital\"]], on=\"Timestamp\", how=\"left\")\n",
    "#     df_curves = pd.merge(df_curves, trades_df[[\"Timestamp_entry\", \"entry_price\"]], on=\"Timestamp_entry\", how=\"left\")\n",
    "#     df_curves[\"Capital\"] = df_curves[\"Capital\"].ffill().fillna(backtester.capital_init)\n",
    "\n",
    "#     timestamps = df_curves[\"Timestamp\"]\n",
    "#     close_prices = df_curves[\"Close\"]\n",
    "#     capital_curve = df_curves[\"Capital\"]\n",
    "#     buy_time = df_curves[\"Timestamp_entry\"]\n",
    "#     buy_price = df_curves[\"entry_price\"]\n",
    "#     sell_time = df_curves[\"Timestamp\"]\n",
    "#     sell_price = df_curves[\"exit_price\"]\n",
    "\n",
    "#     # Créer un subplot avec 2 graphiques (prix en haut, capital en bas)\n",
    "#     fig = make_subplots(\n",
    "#         rows=2, cols=1,\n",
    "#         shared_xaxes=True,\n",
    "#         vertical_spacing=0.1,\n",
    "#         subplot_titles=('Cours Close avec signaux Buy/Sell', 'Évolution du Capital'),\n",
    "#         row_heights=[0.6, 0.4]\n",
    "#     )\n",
    "\n",
    "#     # Graphique 1 : Prix avec signaux\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=timestamps,\n",
    "#             y=close_prices,\n",
    "#             mode='lines',\n",
    "#             name='Close',\n",
    "#             line=dict(color='blue')\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=buy_time,\n",
    "#             y=buy_price,\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='green', symbol='triangle-up', size=10),\n",
    "#             name='Buy'\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=sell_time,\n",
    "#             y=sell_price,\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='red', symbol='triangle-down', size=10),\n",
    "#             name='Sell'\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "#     # Graphique 2 : Capital\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=timestamps,\n",
    "#             y=capital_curve,\n",
    "#             mode='lines',\n",
    "#             name='Capital',\n",
    "#             line=dict(color='purple', width=2)\n",
    "#         ),\n",
    "#         row=2, col=1\n",
    "#     )\n",
    "\n",
    "#     # Ligne de référence pour le capital initial\n",
    "#     fig.add_hline(\n",
    "#         y=backtester.capital_init,\n",
    "#         line_dash=\"dash\",\n",
    "#         line_color=\"gray\",\n",
    "#         annotation_text=f\"Capital initial: {backtester.capital_init:.2f}\",\n",
    "#         row=2, col=1\n",
    "#     )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title='Cours Close avec signaux Buy/Sell et Évolution du Capital',\n",
    "#         height=800,\n",
    "#         width=1200,\n",
    "#         showlegend=True,\n",
    "#         legend=dict(x=0, y=1)\n",
    "#     )\n",
    "\n",
    "#     fig.update_xaxes(title_text=\"Timestamp\", row=2, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Prix\", row=1, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Capital\", row=2, col=1)\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a340a8e",
   "metadata": {},
   "source": [
    "## 2) Chargement des données Binance (ccxt) + cache local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7e067c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                  Timestamp     Open     High      Low    Close     Volume\n",
       " 0 2018-12-15 03:00:00+00:00  3200.00  3312.32  3000.00  3225.97   2.374006\n",
       " 1 2018-12-15 04:00:00+00:00  3225.97  3228.10  3205.58  3228.10   2.410518\n",
       " 2 2018-12-15 05:00:00+00:00  3228.10  3228.10  3204.06  3222.87   3.514068\n",
       " 3 2018-12-15 06:00:00+00:00  3225.68  3225.80  3199.87  3199.87   2.220411\n",
       " 4 2018-12-15 07:00:00+00:00  3199.88  3220.15  3191.44  3205.42  46.164846,\n",
       "                       Timestamp      Open      High       Low     Close  \\\n",
       " 57427 2025-12-17 13:00:00+00:00  87009.40  87837.43  86800.00  87605.29   \n",
       " 57428 2025-12-17 14:00:00+00:00  87605.29  89673.26  87139.53  89653.85   \n",
       " 57429 2025-12-17 15:00:00+00:00  89653.85  90352.81  87124.00  87222.12   \n",
       " 57430 2025-12-17 16:00:00+00:00  87213.00  87759.81  86144.49  86964.42   \n",
       " 57431 2025-12-17 17:00:00+00:00  86964.42  87085.00  86237.26  86550.76   \n",
       " \n",
       "            Volume  \n",
       " 57427   308.98080  \n",
       " 57428   768.17850  \n",
       " 57429  1167.59820  \n",
       " 57430   902.95314  \n",
       " 57431   242.29440  ,\n",
       " (57432, 6))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import ccxt\n",
    "from datetime import datetime, timezone\n",
    "from pandas import to_datetime\n",
    "\n",
    "def to_ms(dt: datetime) -> int:\n",
    "    return int(dt.replace(tzinfo=timezone.utc).timestamp() * 1000)\n",
    "\n",
    "def fetch_ohlcv_binance(pair: str, timeframe: str, start_date: int, end_date: int, limit=1000):\n",
    "    ex = ccxt.binance({\"enableRateLimit\": True})\n",
    "    all_rows = []\n",
    "    since = to_ms(to_datetime(start_date))\n",
    "    end_ms = to_ms(to_datetime(end_date))\n",
    "\n",
    "    while since < end_ms:\n",
    "        batch = ex.fetch_ohlcv(pair, timeframe=timeframe, since=since, limit=limit)\n",
    "        if not batch:\n",
    "            break\n",
    "        all_rows.extend(batch)\n",
    "        # avance d'un pas après le dernier timestamp\n",
    "        since = batch[-1][0] + 1\n",
    "\n",
    "        # sécurité anti-boucle\n",
    "        if len(batch) < 10:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop_duplicates(subset=[\"Timestamp\"]).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "CACHE = f\"btc_usdc_{TIMEFRAME}_{START_DATE}_{END_DATE}.csv\"\n",
    "if os.path.isfile(CACHE):\n",
    "    df = pd.read_csv(CACHE, parse_dates=[\"Timestamp\"])\n",
    "    if df[\"Timestamp\"].dt.tz is None:\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].dt.tz_localize(\"UTC\")\n",
    "else:\n",
    "    df = fetch_ohlcv_binance(PAIR, TIMEFRAME, START_DATE, END_DATE)\n",
    "    df.to_csv(CACHE, index=False)\n",
    "\n",
    "df.head(), df.tail(), df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09202aa9",
   "metadata": {},
   "source": [
    "## 3) Préparation : nettoyage, features RF, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "282d7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     df = df.copy()\n",
    "#     df = df.dropna().reset_index(drop=True)\n",
    "#     return df\n",
    "\n",
    "# def calculate_features_pct_change(df):\n",
    "#     df = df.copy()\n",
    "#     df[\"Close_pct_change\"] = df[\"Close\"].pct_change()\n",
    "#     df[\"High_pct_change\"] = df[\"High\"].pct_change()\n",
    "#     df[\"Low_pct_change\"] = df[\"Low\"].pct_change()          \n",
    "#     df[\"Volume_pct_change\"] = df[\"Volume\"].pct_change()\n",
    "#     features_cols = [\"Close_pct_change\", \"High_pct_change\", \"Low_pct_change\", \"Volume_pct_change\"]\n",
    "#     return df, features_cols\n",
    "\n",
    "# def calculate_features_technical(df):\n",
    "#     df = df.copy()\n",
    "#     close = df[\"Close\"]\n",
    "#     high = df[\"High\"]\n",
    "#     low  = df[\"Low\"]\n",
    "#     vol  = df[\"Volume\"]\n",
    "#     open = df[\"Open\"]\n",
    "\n",
    "#     df[\"logret_1\"] = np.log(close).diff()\n",
    "#     df[\"logret_5\"] = np.log(close).diff(5)\n",
    "#     df[\"logret_20\"] = np.log(close).diff(20)\n",
    "\n",
    "#     df[\"vol_20\"] = df[\"logret_1\"].rolling(20).std()\n",
    "#     df[\"vol_50\"] = df[\"logret_1\"].rolling(50).std()\n",
    "\n",
    "#     df[\"ma20\"] = close.rolling(20).mean()\n",
    "#     df[\"ma50\"] = close.rolling(50).mean()\n",
    "#     df[\"ema20\"] = close.ewm(span=20, adjust=False).mean()\n",
    "#     df[\"ema50\"] = close.ewm(span=50, adjust=False).mean()\n",
    "\n",
    "#     df[\"ma_diff\"] = df[\"ma20\"] - df[\"ma50\"]\n",
    "#     df[\"ema_diff\"] = df[\"ema20\"] - df[\"ema50\"]\n",
    "\n",
    "#     df[\"rsi14\"] = ta.momentum.rsi(close, window=14)\n",
    "#     macd = ta.trend.MACD(close)\n",
    "#     df[\"macd\"] = macd.macd()\n",
    "#     df[\"macd_signal\"] = macd.macd_signal()\n",
    "\n",
    "#     df[\"atr14\"] = ta.volatility.average_true_range(high, low, close, window=14) / close\n",
    "#     df[\"adx14\"] = ta.trend.adx(high, low, close, window=14)\n",
    "\n",
    "#     df[\"hl_range\"] = (high - low) / close\n",
    "#     df[\"np_range\"] = (high - low) / (close - open+1e-6)\n",
    "#     features_cols = []\n",
    "\n",
    "#     # On retire les lignes avec NaNs (features + futur)\n",
    "#     features_cols = [\"logret_1\", \"logret_5\", \"logret_20\", \"vol_20\", \n",
    "#     \"vol_50\", \"ma20\", \"ma50\", \"ema20\", \"ema50\", \"ma_diff\", \"ema_diff\", \n",
    "#     \"rsi14\", \"macd\", \"macd_signal\", \"atr14\", \"adx14\", \"hl_range\", \"np_range\"]\n",
    "    \n",
    "#     return df, features_cols\n",
    "\n",
    "# def calculate_label(df):\n",
    "#     df = df.copy()\n",
    "#     # --- Label : ROI futur à HORIZON_STEPS ---\n",
    "#     df[\"future_close\"] = df[\"Close\"].shift(-HORIZON_STEPS)\n",
    "#     df[\"roi_H\"] = (df[\"future_close\"] - df[\"Close\"]) / df[\"Close\"]\n",
    "#     df[\"y\"] = (df[\"roi_H\"] > THRESH).astype(int)\n",
    "#     return df\n",
    "\n",
    "# def prepare_data_min_features(df):\n",
    "#     # Nettoyage basique\n",
    "#     df = clean_data(df)\n",
    "#     df, features_cols = calculate_features_pct_change(df)\n",
    "#     df = calculate_label(df)\n",
    "#     df_model = df.dropna(subset=features_cols + [\"y\"]).reset_index(drop=True)\n",
    "#     df_model[\"Volume_pct_change\"] = df_model[\"Volume_pct_change\"].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "#     return df_model, features_cols\n",
    "\n",
    "\n",
    "# def prepare_data_advanced_features(df):\n",
    "#     # Nettoyage basique\n",
    "#     df = clean_data(df)\n",
    "#     df,features_cols = calculate_features_technical(df)\n",
    "#     df = calculate_label(df)\n",
    "#     df_model = df.dropna(subset=features_cols + [\"y\"]).reset_index(drop=True)\n",
    "#     return df_model, features_cols\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0246f",
   "metadata": {},
   "source": [
    "## 4) Baseline — RandomForestClassifier (Feature mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "590caa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (45944, 13)\n",
      "valid_df.shape: (5743, 13)\n",
      "test_df.shape: (5744, 13)\n",
      "df_model['y'].value_counts(normalize=True): y\n",
      "0    0.522732\n",
      "1    0.477268\n",
      "Name: proportion, dtype: float64\n",
      "Train/Valid/Test ratio: 45944/51687/5744\n",
      "RandomForest — Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5501    0.7030    0.6172      3155\n",
      "           1     0.4527    0.2993    0.3604      2589\n",
      "\n",
      "    accuracy                         0.5211      5744\n",
      "   macro avg     0.5014    0.5012    0.4888      5744\n",
      "weighted avg     0.5062    0.5211    0.5015      5744\n",
      "\n",
      "Confusion matrix:\n",
      " [[2218  937]\n",
      " [1814  775]]\n",
      "ROC-AUC: 0.5040921024522254\n",
      "PR-AUC: 0.46070281008843295\n"
     ]
    }
   ],
   "source": [
    "df_model,feature_cols = prepare_data_min_features(df,HORIZON_STEPS,THRESH)\n",
    "# Split temporel\n",
    "n = len(df_model)\n",
    "train_end = int(n * TRAIN_RATIO)\n",
    "valid_end = int(n * (TRAIN_RATIO + VALID_RATIO))\n",
    "\n",
    "train_df = df_model.iloc[:train_end].copy()\n",
    "valid_df = df_model.iloc[train_end:valid_end].copy()\n",
    "test_df  = df_model.iloc[valid_end:].copy()\n",
    "\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"valid_df.shape: {valid_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")\n",
    "print(f\"df_model['y'].value_counts(normalize=True): {df_model['y'].value_counts(normalize=True)}\")\n",
    "print(f\"Train/Valid/Test ratio: {train_end}/{valid_end}/{n-valid_end}\")\n",
    "\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[\"y\"].values\n",
    "\n",
    "X_valid = valid_df[feature_cols].values\n",
    "y_valid = valid_df[\"y\"].values\n",
    "\n",
    "X_test  = test_df[feature_cols].values\n",
    "y_test  = test_df[\"y\"].values\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Probabilités pour AUC/PR-AUC\n",
    "p_test = rf.predict_proba(X_test)[:,1]\n",
    "pred_test_rf_simple = (p_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"RandomForest — Classification report (test):\")\n",
    "print(classification_report(y_test, pred_test_rf_simple, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_test_rf_simple)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "try:\n",
    "    roc = roc_auc_score(y_test, p_test)\n",
    "    print(\"ROC-AUC:\", roc)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC non calculable:\", e)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, p_test)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR-AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "13237430",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'scoring' parameter of GridSearchCV must be a str among {'fowlkes_mallows_score', 'balanced_accuracy', 'jaccard_micro', 'top_k_accuracy', 'mutual_info_score', 'adjusted_mutual_info_score', 'r2', 'precision_samples', 'jaccard_samples', 'precision_micro', 'f1_weighted', 'precision_weighted', 'neg_root_mean_squared_error', 'rand_score', 'recall_samples', 'homogeneity_score', 'neg_brier_score', 'completeness_score', 'average_precision', 'f1_samples', 'neg_median_absolute_error', 'd2_absolute_error_score', 'recall_weighted', 'normalized_mutual_info_score', 'precision', 'recall', 'recall_macro', 'accuracy', 'jaccard', 'neg_log_loss', 'f1', 'jaccard_macro', 'f1_micro', 'neg_mean_squared_error', 'neg_negative_likelihood_ratio', 'precision_macro', 'neg_mean_absolute_error', 'adjusted_rand_score', 'jaccard_weighted', 'roc_auc_ovr_weighted', 'matthews_corrcoef', 'v_measure_score', 'neg_root_mean_squared_log_error', 'recall_micro', 'positive_likelihood_ratio', 'roc_auc_ovo', 'roc_auc_ovr', 'f1_macro', 'neg_mean_squared_log_error', 'neg_mean_gamma_deviance', 'neg_max_error', 'roc_auc', 'roc_auc_ovo_weighted', 'neg_mean_poisson_deviance', 'explained_variance', 'neg_mean_absolute_percentage_error'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 'roc-auc' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# GridSearch avec CV temporel\u001b[39;00m\n\u001b[32m     20\u001b[39m grid = GridSearchCV(\n\u001b[32m     21\u001b[39m     estimator=rf_model,\n\u001b[32m     22\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     verbose=\u001b[32m2\u001b[39m,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest RF params via GridSearchCV/TS-CV :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(grid.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Formation OpenClassRooms Ingénieur IA/P09/.venv/lib/python3.11/site-packages/sklearn/base.py:1358\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1353\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1354\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1355\u001b[39m )\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Formation OpenClassRooms Ingénieur IA/P09/.venv/lib/python3.11/site-packages/sklearn/base.py:471\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    465\u001b[39m \n\u001b[32m    466\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Formation OpenClassRooms Ingénieur IA/P09/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'scoring' parameter of GridSearchCV must be a str among {'fowlkes_mallows_score', 'balanced_accuracy', 'jaccard_micro', 'top_k_accuracy', 'mutual_info_score', 'adjusted_mutual_info_score', 'r2', 'precision_samples', 'jaccard_samples', 'precision_micro', 'f1_weighted', 'precision_weighted', 'neg_root_mean_squared_error', 'rand_score', 'recall_samples', 'homogeneity_score', 'neg_brier_score', 'completeness_score', 'average_precision', 'f1_samples', 'neg_median_absolute_error', 'd2_absolute_error_score', 'recall_weighted', 'normalized_mutual_info_score', 'precision', 'recall', 'recall_macro', 'accuracy', 'jaccard', 'neg_log_loss', 'f1', 'jaccard_macro', 'f1_micro', 'neg_mean_squared_error', 'neg_negative_likelihood_ratio', 'precision_macro', 'neg_mean_absolute_error', 'adjusted_rand_score', 'jaccard_weighted', 'roc_auc_ovr_weighted', 'matthews_corrcoef', 'v_measure_score', 'neg_root_mean_squared_log_error', 'recall_micro', 'positive_likelihood_ratio', 'roc_auc_ovo', 'roc_auc_ovr', 'f1_macro', 'neg_mean_squared_log_error', 'neg_mean_gamma_deviance', 'neg_max_error', 'roc_auc', 'roc_auc_ovo_weighted', 'neg_mean_poisson_deviance', 'explained_variance', 'neg_mean_absolute_percentage_error'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 'roc-auc' instead."
     ]
    }
   ],
   "source": [
    "# Ajoute gridSearch cv sur tscv sur les hyperparamètres de RF\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Définition des hyperparamètres à explorer\n",
    "param_grid = {\n",
    "    'n_estimators': [800],\n",
    "    'max_depth': [10, 20, 25],\n",
    "    'min_samples_leaf': [10,50],\n",
    "    'class_weight': ['balanced'],\n",
    "    'random_state': [RANDOM_SEED]\n",
    "}\n",
    "\n",
    "# TimeSeries cross-validator \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# GridSearch avec CV temporel\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='roc-auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params via GridSearchCV/TS-CV :\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best RF score:\", grid.best_score_)\n",
    "\n",
    "# Vous pouvez choisir le meilleur modèle ainsi:\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Et l'utiliser comme à la place de rf :\n",
    "# best_rf.fit(X_train, y_train)\n",
    "# etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f748d8",
   "metadata": {},
   "source": [
    "### Backtest simple (baseline RF)\n",
    "Stratégie : **spot long-only**. Si prédiction=1 → être long sur l’horizon (ici simplifié pas-à-pas : position à 1 tant que le signal vaut 1). Frais appliqués lors des changements de position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester_rf_simple = Backtest(test_df.reset_index(), pred_test_rf_simple, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_backtest(backtester_rf_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a0251",
   "metadata": {},
   "source": [
    "## 4bis) RandomForet feature advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model,feature_cols = prepare_data_advanced_features(df,HORIZON_STEPS,THRESH)\n",
    "# Split temporel\n",
    "n = len(df_model)\n",
    "train_end = int(n * TRAIN_RATIO)\n",
    "valid_end = int(n * (TRAIN_RATIO + VALID_RATIO))\n",
    "\n",
    "train_df = df_model.iloc[:train_end].copy()\n",
    "valid_df = df_model.iloc[train_end:valid_end].copy()\n",
    "test_df  = df_model.iloc[valid_end:].copy()\n",
    "\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"valid_df.shape: {valid_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")\n",
    "print(f\"df_model['y'].value_counts(normalize=True): {df_model['y'].value_counts(normalize=True)}\")\n",
    "print(f\"Train/Valid/Test ratio: {train_end}/{valid_end}/{n-valid_end}\")\n",
    "\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[\"y\"].values\n",
    "\n",
    "X_valid = valid_df[feature_cols].values\n",
    "y_valid = valid_df[\"y\"].values\n",
    "\n",
    "X_test  = test_df[feature_cols].values\n",
    "y_test  = test_df[\"y\"].values\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Probabilités pour AUC/PR-AUC\n",
    "p_test = rf.predict_proba(X_test)[:,1]\n",
    "pred_test_rf_advanced = (p_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"RandomForest — Classification report (test):\")\n",
    "print(classification_report(y_test, pred_test_rf_advanced, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_test_rf_advanced)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "try:\n",
    "    roc = roc_auc_score(y_test, p_test)\n",
    "    print(\"ROC-AUC:\", roc)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC non calculable:\", e)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, p_test)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR-AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933000d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester_rf_advanced = Backtest(test_df.reset_index(), pred_test_rf_advanced, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_backtest(backtester_rf_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166d9ae",
   "metadata": {},
   "source": [
    "## 5) Modèle récent — TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21328a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_model,feature_cols = prepare_data_min_features(df,HORIZON_STEPS,THRESH)\n",
    "# Split temporel\n",
    "n = len(df_model)\n",
    "train_end = int(n * TRAIN_RATIO)\n",
    "valid_end = int(n * (TRAIN_RATIO + VALID_RATIO))\n",
    "\n",
    "train_df = df_model.iloc[:train_end].copy()\n",
    "valid_df = df_model.iloc[train_end:valid_end].copy()\n",
    "test_df  = df_model.iloc[valid_end:].copy()\n",
    "\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"valid_df.shape: {valid_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")\n",
    "print(f\"df_model['y'].value_counts(normalize=True): {df_model['y'].value_counts(normalize=True)}\")\n",
    "print(f\"Train/Valid/Test ratio: {train_end}/{valid_end}/{n-valid_end}\")\n",
    "\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[\"y\"].values\n",
    "\n",
    "X_valid = valid_df[feature_cols].values\n",
    "y_valid = valid_df[\"y\"].values\n",
    "\n",
    "X_test  = test_df[feature_cols].values\n",
    "y_test  = test_df[\"y\"].values\n",
    "\n",
    "# Scaling (fit sur train uniquement) - CRITIQUE pour TabNet\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_valid_s = scaler.transform(X_valid)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "y_train_tab = y_train\n",
    "y_valid_tab = y_valid\n",
    "y_test_tab  = y_test\n",
    "\n",
    "# Retour aux hyperparamètres qui fonctionnaient mieux (TEST 1 était pire)\n",
    "tabnet = TabNetClassifier(\n",
    "    n_d=32, n_a=32, \n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-3),\n",
    "    mask_type=\"sparsemax\",\n",
    "    seed=RANDOM_SEED,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "tabnet.fit(\n",
    "    X_train_s, y_train_tab,\n",
    "    eval_set=[(X_valid_s, y_valid_tab)],\n",
    "    eval_name=[\"valid\"],\n",
    "    eval_metric=[\"auc\"],\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=256,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "# Évaluation classification (test)\n",
    "p_test_tab = tabnet.predict_proba(X_test_s)[:, 1]\n",
    "pred_test_tab = (p_test_tab >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TabNet - Classification report (test)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_tab, pred_test_tab, digits=4))\n",
    "\n",
    "cm_tab = confusion_matrix(y_test_tab, pred_test_tab)\n",
    "print(\"Confusion matrix:\\n\", cm_tab)\n",
    "\n",
    "try:\n",
    "    roc_tab = roc_auc_score(y_test_tab, p_test_tab)\n",
    "    print(\"ROC-AUC:\", roc_tab)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC non calculable:\", e)\n",
    "\n",
    "prec_tab, rec_tab, _ = precision_recall_curve(y_test_tab, p_test_tab)\n",
    "pr_auc_tab = auc(rec_tab, prec_tab)\n",
    "print(\"PR-AUC:\", pr_auc_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c41761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle TabNet au format pkl\n",
    "joblib.dump(tabnet, \"models/tabnet_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fedfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = joblib.load(\"models/tabnet_model.pkl\")    \n",
    "\n",
    "# Évaluation classification (test)\n",
    "p_test_tab = tabnet.predict_proba(X_test_s)[:, 1]\n",
    "pred_test_tab = (p_test_tab >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TabNet - Classification report (test)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_tab, pred_test_tab, digits=4))\n",
    "\n",
    "cm_tab = confusion_matrix(y_test_tab, pred_test_tab)\n",
    "print(\"Confusion matrix:\\n\", cm_tab)\n",
    "\n",
    "try:\n",
    "    roc_tab = roc_auc_score(y_test_tab, p_test_tab)\n",
    "    print(\"ROC-AUC:\", roc_tab)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC non calculable:\", e)\n",
    "\n",
    "prec_tab, rec_tab, _ = precision_recall_curve(y_test_tab, p_test_tab)\n",
    "pr_auc_tab = auc(rec_tab, prec_tab)\n",
    "print(\"PR-AUC:\", pr_auc_tab)\n",
    "backtester_tabnet = Backtest(test_df.reset_index(), pred_test_tab, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n",
    "plot_backtest(backtester_tabnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule — Tableau comparaison RF vs TabNet (version simple, basée sur capital/PnL)\n",
    "capital_init = CAPITAL_INIT\n",
    "\n",
    "# RF (si tu utilises déjà `backtester` pour RF)\n",
    "pnl_rf_simple = backtester_rf_simple.capital - capital_init\n",
    "pnl_rf_advanced = backtester_rf_advanced.capital - capital_init\n",
    "pnl_tabnet = backtester_tabnet.capital - capital_init\n",
    "\n",
    "results_bt = pd.DataFrame([\n",
    "    {\"Model\": \"RandomForest Simple\", \"CapitalFinal\": backtester_rf_simple.capital, \"PnL\": pnl_rf_simple},\n",
    "    {\"Model\": \"RandomForest Advanced\", \"CapitalFinal\": backtester_rf_advanced.capital, \"PnL\": pnl_rf_advanced},\n",
    "    {\"Model\": \"TabNet\", \"CapitalFinal\": backtester_tabnet.capital, \"PnL\": pnl_tabnet},\n",
    "])\n",
    "\n",
    "results_bt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfb25e",
   "metadata": {},
   "source": [
    "## 6) Modèle récent — Tiny Time Mixer (TTM) : forecasting → signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd029f46",
   "metadata": {},
   "source": [
    "On reproduit l’approche du notebook Kaggle **Tiny Time Mixer** (TSFM) :  \n",
    "- on prépare le dataset pour forecasting (cible = `Close`)  \n",
    "- on fine-tune léger TTM (optionnel, mais recommandé)  \n",
    "- on produit une prévision à `HORIZON_STEPS` et on convertit en signal :  \n",
    "`Buy` si ROI prédit > seuil (frais).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d83737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure tsfm_public is installed in the current Python environment\n",
    "# import sys\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     import tsfm_public\n",
    "#     print(f\"✓ tsfm_public is installed (version: {tsfm_public.__version__})\")\n",
    "# except ImportError:\n",
    "#     print(\"Installing tsfm_public in current Python environment...\")\n",
    "#     TSFM_DIR = \"tsfm\"\n",
    "#     if not os.path.isdir(TSFM_DIR):\n",
    "#         subprocess.check_call([\"bash\",\"-lc\", f\"git clone --depth 1 --branch v0.2.9 https://github.com/IBM/tsfm.git {TSFM_DIR}\"])\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"-e\", TSFM_DIR])\n",
    "#     print(\"✓ Installation complete. Please restart the kernel and re-run this cell.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65aa85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import from submodules directly (following official notebook pattern)\n",
    "# # Import from specific modules to avoid circular import issues\n",
    "# from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor, get_datasets\n",
    "# from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "# from tsfm_public.toolkit.time_series_forecasting_pipeline import TimeSeriesForecastingPipeline\n",
    "\n",
    "# # Import transformers utilities\n",
    "# from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# print(\"✓ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da09b8",
   "metadata": {},
   "source": [
    "### 6.1) Préparation des données pour TTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48415c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Pour rester simple : TTM voit Open/High/Low comme observables, Close comme target\n",
    "# timestamp_column = \"Timestamp\"\n",
    "# target_columns = [\"nextClose_pct_change\"]\n",
    "\n",
    "# df_model,feature_cols = prepare_data_min_features(df,HORIZON_STEPS,THRESH)\n",
    "# df_model[\"nextClose_pct_change\"] = (df_model[\"Close\"].shift(-1) -df_model[\"Close\"]) / df_model[\"Close\"]\n",
    "# observable_columns = feature_cols\n",
    "\n",
    "# # On prend df_model (brut) mais on se limite à la zone où le futur existe (sinon label NaN)\n",
    "# df_ttm = df_model.dropna(subset=observable_columns+target_columns).copy()\n",
    "\n",
    "# # Préprocesseur TSFM (semblable au notebook Kaggle)\n",
    "# # Note: context_length et prediction_length sont passés au preprocessor\n",
    "# # Le modèle pré-entraîné utilise prediction_length=96, on doit l'utiliser ici aussi\n",
    "# # On utilisera ensuite seulement les HORIZON_STEPS premiers pas pour le signal de trading\n",
    "# tsp = TimeSeriesPreprocessor(\n",
    "#     timestamp_column=timestamp_column,\n",
    "#     id_columns=[],\n",
    "#     target_columns=target_columns,\n",
    "#     observable_columns=observable_columns,\n",
    "#     freq=TIMEFRAME,\n",
    "#     context_length=LOOKBACK,\n",
    "#     prediction_length=96  # Doit correspondre au modèle pré-entraîné\n",
    "# )\n",
    "\n",
    "# # Split indices sur df_ttm (chronologique)\n",
    "# data_length = len(df_ttm)\n",
    "# train_start_index = 0\n",
    "# train_end_index = round(data_length * TRAIN_RATIO)\n",
    "\n",
    "# eval_start_index = train_end_index - LOOKBACK\n",
    "# eval_end_index   = round(data_length * (TRAIN_RATIO + VALID_RATIO))\n",
    "\n",
    "# test_start_index = eval_end_index - LOOKBACK\n",
    "# test_end_index   = data_length\n",
    "\n",
    "# split_config = {\n",
    "#     \"train\": [train_start_index, train_end_index],\n",
    "#     \"valid\": [eval_start_index, eval_end_index],\n",
    "#     \"test\":  [test_start_index, test_end_index],\n",
    "# }\n",
    "\n",
    "# train_dataset, valid_dataset, test_dataset = get_datasets(\n",
    "#     tsp,\n",
    "#     df_ttm,\n",
    "#     split_config=split_config\n",
    "# )\n",
    "\n",
    "# print(f\"train_dataset: {len(    train_dataset)}\")\n",
    "# print(f\"valid_dataset: {len(valid_dataset)}\")\n",
    "# print(f\"test_dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ffe7d",
   "metadata": {},
   "source": [
    "### 6.2) Chargement du modèle TTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checkpoint public utilisé par TSFM (le notebook Kaggle utilise TinyTimeMixerForPrediction)\n",
    "# Remarque : selon l'environnement, le téléchargement HF peut prendre du temps.\n",
    "# Le checkpoint est entraîné avec prediction_length=96, on doit l'utiliser tel quel\n",
    "# On utilisera ensuite seulement les HORIZON_STEPS premiers pas de la prédiction\n",
    "model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-r2\",  # checkpoint TTM (prediction_length=96)\n",
    "    context_length=LOOKBACK,\n",
    "    prediction_length=96  # Le checkpoint utilise 96, on doit respecter cette valeur\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"✓ Modèle chargé sur {device}\")\n",
    "print(f\"  - context_length: {model.config.context_length}\")\n",
    "print(f\"  - prediction_length: {model.config.prediction_length}\")\n",
    "print(f\"  - On utilisera les {HORIZON_STEPS} premiers pas de la prédiction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ceb27",
   "metadata": {},
   "source": [
    "### 6.3) Fine-tuning léger (optionnel mais recommandé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a969eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement léger sur CPU possible mais plus lent.\n",
    "# Vous pouvez passer SKIP_FINETUNE=True pour tester rapidement en zero-shot.\n",
    "\n",
    "SKIP_FINETUNE = False\n",
    "\n",
    "if not SKIP_FINETUNE:\n",
    "    # Arguments d'entraînement\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"ttm_finetune_out\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        logging_steps=50,\n",
    "        seed=RANDOM_SEED,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    steps_per_epoch = max(1, len(train_dataset) // training_args.per_device_train_batch_size)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=training_args.learning_rate,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=training_args.num_train_epochs,\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        callbacks=[early_stopping],\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828cfd54",
   "metadata": {},
   "source": [
    "### 6.4) Prédiction sur test → signal Buy/No-trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec470ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle TinyTimeMixer pour une réutilisation en production\n",
    "\n",
    "import torch\n",
    "\n",
    "# Sauvegarde complète de l'état du modèle (architecture + poids)\n",
    "#torch.save(model, \"ttm_model_full.pt\")\n",
    "\n",
    "# OU, plus communément en production :\n",
    "# Sauvegarde seulement des poids (state_dict)\n",
    "torch.save(model.state_dict(), \"ttm_model_state_dict.pt\")\n",
    "\n",
    "# Pour recharger plus tard :\n",
    "# - Recharger tout (si architecture inchangée) :\n",
    "#   model = torch.load(\"ttm_model_full.pt\")\n",
    "# - OU, pour charger les poids dans une nouvelle instance :\n",
    "#   model = TinyTimeMixerForPrediction(...)  # instancier avec la même architecture\n",
    "#   model.load_state_dict(torch.load(\"ttm_model_state_dict.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forecast_pipeline = TimeSeriesForecastingPipeline(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=[],\n",
    "    target_columns=target_columns,\n",
    "    observable_columns=observable_columns,\n",
    "    freq=TIMEFRAME\n",
    ")\n",
    "\n",
    "# Prévisions : on obtient une table avec Close réel et Close_prediction (format pipeline TSFM)\n",
    "ttm_forecast = forecast_pipeline(tsp.preprocess(df_ttm.iloc[test_start_index:test_end_index].copy()))\n",
    "ttm_forecast.head(), ttm_forecast.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traçage de la courbe Close réel et Close_prediction, gestion des séquences éventuelles (list/array)\n",
    "def extract_scalar_for_plot(x, index=0):\n",
    "    try:\n",
    "        if isinstance(x, (list, tuple, np.ndarray)):\n",
    "            if len(x) > index:\n",
    "                return float(x[index])\n",
    "            elif len(x) > 0:\n",
    "                return float(x[0])\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Extraction sécurisée des valeurs pour l'affichage\n",
    "close_y = [extract_scalar_for_plot(x) for x in ttm_forecast[\"nextClose_pct_change\"]]\n",
    "close_pred_y = [extract_scalar_for_plot(x) for x in ttm_forecast[\"nextClose_pct_change_prediction\"]]\n",
    "timestamps = ttm_forecast[\"Timestamp\"]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(timestamps, close_y, label=\"Close réel\", color=\"blue\")\n",
    "plt.plot(timestamps, close_pred_y, label=\"Close prédit\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597eb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ttm = (pd.DataFrame(close_pred_y) >= 0).astype(int)\n",
    "y_pred_ttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_ttm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643941db",
   "metadata": {},
   "source": [
    "### Backtest simple (TTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60959ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred_ttm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf59e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ttm.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester_ttm = Backtest(test_df.reset_index(), y_pred_ttm.values, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n",
    "plot_backtest(backtester_ttm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e18f0b",
   "metadata": {},
   "source": [
    "## 7 Comparaison synthétique + courbes d'equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98712e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Récupérer les courbes d'equity des différents backtest\n",
    "backtests = [\n",
    "    (\"RandomForest Simple\", backtester_rf_simple),\n",
    "    (\"RandomForest Advanced\", backtester_rf_advanced),\n",
    "    (\"TabNet\", backtester_tabnet),\n",
    "    (\"TTM\", backtester_ttm),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "for name, bt in backtests:\n",
    "    # Capital curve déjà stockée dans backtester.df_trades ou reconstruite depuis les trades si besoin\n",
    "    df_curve = bt.df_bt.copy().reset_index(drop=True)\n",
    "    # On merge le capital atteint lors de chaque trade, puis ffill pour combler entre trades\n",
    "    df_curve = df_curve.merge(bt.df_trades[[\"Timestamp\", \"Capital\"]], on=\"Timestamp\", how=\"left\")\n",
    "    df_curve[\"Capital\"] = df_curve[\"Capital\"].ffill().fillna(bt.capital_init)\n",
    "    plt.plot(df_curve[\"Timestamp\"], df_curve[\"Capital\"], label=name)\n",
    "\n",
    "plt.title(\"Courbes d'equity des différents backtests\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Capital\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c80a5",
   "metadata": {},
   "source": [
    "## 8 Notes pour le dashboard Streamlit\n",
    "- Charger les derniers points OHLCV\n",
    "- Calculer features RF et prédiction RF\n",
    "- Exécuter TabNet pipeline sur la fenêtre récente et générer signal\n",
    "- Afficher signal + courbe prix + métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22538a5a",
   "metadata": {},
   "source": [
    "### Prédiction avec TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les derniers points OHLCV\n",
    "from datetime import timedelta\n",
    "\n",
    "last_data = fetch_ohlcv_binance(\"BTCUSDC\", \"1h\", format(datetime.now()-timedelta(days=7),\"%Y-%m-%d\"), format(datetime.now(),\"%Y-%m-%d\"))\n",
    "# Appliquer le pipeline de préprocessing des données\n",
    "last_data_model, features_cols_model = prepare_data_min_features(last_data,HORIZON_STEPS,THRESH)\n",
    "# Charger le modèle TabNet\n",
    "\n",
    "\n",
    "tabnet_model =joblib.load(\"tabnet_model.pkl\")\n",
    "# Faire les prédictions\n",
    "last_data_model_s  = scaler.transform(last_data_model[feature_cols].values)\n",
    "tabnet_pred_tab = tabnet_model.predict_proba(last_data_model_s)[:, 1]\n",
    "tabnet_pred = (tabnet_pred_tab >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle TabNet\n",
    "tabnet_model =joblib.load(\"tabnet_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2187f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data_model['Timestamp'].iloc[-1].timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(datetime.utcnow()-timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data_model['Timestamp'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff66803",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_datetime(datetime.utcnow()-timedelta(minutes=5),utc=True)>last_data_model['Timestamp'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700562cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester_tabnet_pred = Backtest(last_data_model.reset_index(), tabnet_pred, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n",
    "plot_backtest(backtester_tabnet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f0f39",
   "metadata": {},
   "source": [
    "### Trading live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nous sommes le {format(datetime.now(), '%d/%m/%Y')},  il est {format(datetime.now(), '%H:%M:%S')}\")\n",
    "last_data = fetch_ohlcv_binance(\"BTCUSDC\", \"1h\", format(datetime.now()-timedelta(days=7),\"%Y-%m-%d\"), format(datetime.now(),\"%Y-%m-%d\"))\n",
    "last_data_model, features_cols_model = prepare_data_min_features(last_data,HORIZON_STEPS,THRESH)\n",
    "last_data_model_s  = scaler.transform(last_data_model[feature_cols].values)\n",
    "tabnet_pred_tab = tabnet_model.predict_proba(last_data_model_s)[:, 1]\n",
    "tabnet_pred = (tabnet_pred_tab >= 0.5).astype(int)\n",
    "print(f\"Le cours du Bitcoin est actuellement de {last_data_model['Close'].iloc[-1]} USDC\")\n",
    "if (to_datetime(datetime.utcnow()-timedelta(minutes=5),utc=True)  < last_data_model['Timestamp'].iloc[-1]) :\n",
    "    print(f\"Le signal TabNet pour les 5 premières minutes de l'heure est : {'Buy' if tabnet_pred[-1] == 1 else 'No-trade or Sell'}\")\n",
    "else:\n",
    "    print(f\"Le signal TabNet n'est pas disponible pour trader, il faut attendre {(last_data_model['Timestamp'].iloc[-1]+timedelta(minutes=60)-to_datetime(datetime.utcnow(),utc=True))} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f457d",
   "metadata": {},
   "source": [
    "### Prediction avec TTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les dernières données OHLCV (7 derniers jours)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "last_data = fetch_ohlcv_binance(\"BTCUSDC\", \"1h\",\n",
    "                                format(datetime.now()-timedelta(days=90), \"%Y-%m-%d\"),\n",
    "                                format(datetime.now(), \"%Y-%m-%d\"))\n",
    "\n",
    "# Appliquer le pipeline de préprocessing du TTM (même que dans le notebook)\n",
    "# Le TTM utilise prepare_data_min_features, pas advanced_features\n",
    "last_data_TTM, features_cols_TTM = prepare_data_min_features(last_data,HORIZON_STEPS,THRESH)\n",
    "\n",
    "# Ajouter la colonne nextClose_pct_change (requise par le preprocessor TTM)\n",
    "# Pour les prédictions, on peut mettre NaN ou 0, car on ne l'utilise pas\n",
    "last_data_TTM[\"nextClose_pct_change\"] = (last_data_TTM[\"Close\"].shift(-1) - last_data_TTM[\"Close\"]) / last_data_TTM[\"Close\"]\n",
    "\n",
    "# Définir les colonnes pour le preprocessor TTM (même configuration que dans le notebook)\n",
    "timestamp_column = \"Timestamp\"\n",
    "target_columns = [\"nextClose_pct_change\"]\n",
    "observable_columns = features_cols_TTM\n",
    "\n",
    "# Créer le preprocessor TTM (même configuration que dans le notebook)\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=[],\n",
    "    target_columns=target_columns,\n",
    "    observable_columns=observable_columns,\n",
    "    freq=TIMEFRAME,\n",
    "    context_length=LOOKBACK,\n",
    "    prediction_length=96  # Doit correspondre au modèle pré-entraîné\n",
    ")\n",
    "\n",
    "# Charger le modèle TTM sauvegardé\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ttm_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-r2\",  # checkpoint TTM (prediction_length=96)\n",
    "    context_length=LOOKBACK,\n",
    "    prediction_length=96  # Le checkpoint utilise 96, on doit respecter cette valeur\n",
    ")\n",
    "ttm_model.load_state_dict(torch.load(\"ttm_model_state_dict.pt\"))\n",
    "ttm_model.to(device)\n",
    "ttm_model.eval()  # Mode évaluation\n",
    "\n",
    "# Créer le pipeline de forecasting\n",
    "forecast_pipeline = TimeSeriesForecastingPipeline(\n",
    "    model=ttm_model,\n",
    "    device=device,\n",
    "    timestamp_column=timestamp_column,\n",
    "    id_columns=[],\n",
    "    target_columns=target_columns,\n",
    "    observable_columns=observable_columns,\n",
    "    freq=TIMEFRAME\n",
    ")\n",
    "\n",
    "# Faire les prédictions sur les dernières données\n",
    "# Le pipeline nécessite des données préprocessées\n",
    "last_data_preprocessed = tsp.preprocess(last_data_TTM.copy())\n",
    "ttm_forecast = forecast_pipeline(last_data_preprocessed)\n",
    "\n",
    "# Extraction sécurisée des valeurs pour l'affichage\n",
    "close_pred_y = [extract_scalar_for_plot(x) for x in ttm_forecast[\"nextClose_pct_change_prediction\"]]\n",
    "timestamps = ttm_forecast[\"Timestamp\"]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(timestamps, close_pred_y, label=\"Close prédit\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_ttm = (pd.DataFrame(close_pred_y) >= 0).astype(int)\n",
    "last_data_TTM_pred = last_data_TTM.tail(24*7).copy()\n",
    "y_pred_ttm_pred = y_pred_ttm.tail(24*7)\n",
    "print(f\"y_pred_ttm_pred.shape: {y_pred_ttm_pred.shape}\")\n",
    "print(f\"last_data_TTM_pred.shape: {last_data_TTM_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtester_ttm = Backtest(last_data_TTM_pred.reset_index(), y_pred_ttm_pred.values, fee_roundtrip=0, pct_capital=1, capital_init=CAPITAL_INIT, debug=False)\n",
    "plot_backtest(backtester_ttm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
